{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ee6126-f815-4456-85ca-459177f91cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6248fd03946d409ca67adfe2b4be7295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# Load the model\n",
    "ckpt = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(ckpt)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "# add some tokens like \"</user>\" and </s> to eos ids\n",
    "eos_token_ids += tokenizer.encode(\"</user>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</s>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209737de-4786-467f-b0d1-3c75c2a6cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    logger,\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    "    LlamaSdpaAttention,\n",
    "    LlamaFlashAttention2,\n",
    "    LlamaMLP\n",
    ")\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_flash_attention_utils  import _flash_attention_forward\n",
    "\n",
    "def minis_mlp_forward(self, x):\n",
    "    bsz, q_len, _ = x.size()\n",
    "    chunk_size = self.hidden_size\n",
    "\n",
    "    x_list = list(x.split(chunk_size, dim=1))\n",
    "\n",
    "    output_list = [None for _ in range(len(x_list))]\n",
    "\n",
    "    for i in range(len(x_list)):\n",
    "        x = x_list[i]\n",
    "        x_list[i] = None\n",
    "        output_list[i] = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "    down_proj = torch.cat(output_list, dim=1)\n",
    "\n",
    "    return down_proj\n",
    "    \n",
    "LlamaMLP.forward = minis_mlp_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d56277-e2e4-4873-9c0f-2fbb3a66f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def LlamaAttention_fast_forward(\n",
    "#     self,\n",
    "#     hidden_states: torch.Tensor,\n",
    "#     attention_mask: Optional[torch.Tensor] = None,\n",
    "#     position_ids: Optional[torch.LongTensor] = None,\n",
    "#     past_key_value: Optional[Cache] = None,\n",
    "#     output_attentions: bool = False,\n",
    "#     use_cache: bool = False,\n",
    "#     cache_position: Optional[torch.LongTensor] = None,\n",
    "#     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "#     **kwargs,\n",
    "# ) :\n",
    "#     output_attentions = False\n",
    "\n",
    "#     bsz, q_len, hd = hidden_states.size()\n",
    "#     chunk_size = hd // self.num_key_value_heads\n",
    "#     num_heads = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "#     if not hasattr(self, 'q_proj_list'):\n",
    "#         self.q_proj_list = list((self.q_proj.weight.split(self.head_dim * num_heads, dim=0)))\n",
    "#         # self.q_proj.weight.data.storage().resize_(0)\n",
    "#     if not hasattr(self, 'k_proj_list'):\n",
    "#         self.k_proj_list = list((self.k_proj.weight.split(self.head_dim, dim=0)))\n",
    "#         # self.k_proj.weight.data.storage().resize_(0)\n",
    "#     if not hasattr(self, 'v_proj_list'):\n",
    "#         self.v_proj_list = list((self.v_proj.weight.split(self.head_dim, dim=0)))\n",
    "#         # self.v_proj.weight.data.storage().resize_(0)\n",
    "\n",
    "\n",
    "#     attn_output_list = [None for _ in range((self.num_key_value_heads))]\n",
    "    \n",
    "#     for i in range(self.num_key_value_heads):\n",
    "#         bsz, q_len, hd = hidden_states.size()\n",
    "\n",
    "#         self.q_proj.weight.data = self.q_proj_list[i].data\n",
    "#         self.k_proj.weight.data = self.k_proj_list[i].data\n",
    "#         self.v_proj.weight.data = self.v_proj_list[i].data\n",
    "\n",
    "#         # print(hidden_states.shape, self.q_proj.weight.shape)\n",
    "        \n",
    "#         query_states = self.q_proj(hidden_states)\n",
    "#         key_states = self.k_proj(hidden_states)\n",
    "#         value_states = self.v_proj(hidden_states)\n",
    "\n",
    "#         # Flash attention requires the input to have the shape\n",
    "#         # batch_size x seq_length x head_dim x hidden_dim\n",
    "#         # therefore we just need to keep the original shape\n",
    "#         query_states = query_states.view(bsz, q_len, num_heads, self.head_dim).transpose(1, 2)\n",
    "#         key_states = key_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "#         value_states = value_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "#         if position_embeddings is None:\n",
    "#             logger.warning_once(\n",
    "#                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "#                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "#                 \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "#                 \"removed and `position_embeddings` will be mandatory.\"\n",
    "#             )\n",
    "#             cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "#         else:\n",
    "#             cos, sin = position_embeddings\n",
    "#         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "    \n",
    "#         if past_key_value is not None:\n",
    "#             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "#             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "#             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx + i, cache_kwargs)\n",
    "    \n",
    "#         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
    "#         # to be able to avoid many of these transpose/reshape/view.\n",
    "#         query_states = query_states.transpose(1, 2)\n",
    "#         key_states = key_states.transpose(1, 2)\n",
    "#         value_states = value_states.transpose(1, 2)\n",
    "    \n",
    "#         dropout_rate = self.attention_dropout if self.training else 0.0\n",
    "    \n",
    "#         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
    "#         # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
    "#         # cast them back in the correct dtype just to be sure everything works as expected.\n",
    "#         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
    "#         # in fp32. (LlamaRMSNorm handles it correctly)\n",
    "    \n",
    "#         input_dtype = query_states.dtype\n",
    "#         if input_dtype == torch.float32:\n",
    "#             if torch.is_autocast_enabled():\n",
    "#                 target_dtype = torch.get_autocast_gpu_dtype()\n",
    "#             # Handle the case where the model is quantized\n",
    "#             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n",
    "#                 target_dtype = self.config._pre_quantization_dtype\n",
    "#             else:\n",
    "#                 target_dtype = self.q_proj.weight.dtype\n",
    "    \n",
    "#             logger.warning_once(\n",
    "#                 f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n",
    "#                 f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n",
    "#                 f\" {target_dtype}.\"\n",
    "#             )\n",
    "    \n",
    "#             query_states = query_states.to(target_dtype)\n",
    "#             key_states = key_states.to(target_dtype)\n",
    "#             value_states = value_states.to(target_dtype)\n",
    "    \n",
    "#         attn_output = _flash_attention_forward(\n",
    "#             query_states,\n",
    "#             key_states,\n",
    "#             value_states,\n",
    "#             attention_mask,\n",
    "#             q_len,\n",
    "#             position_ids=position_ids,\n",
    "#             dropout=dropout_rate,\n",
    "#             sliding_window=getattr(self, \"sliding_window\", None),\n",
    "#             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n",
    "#             is_causal=self.is_causal,\n",
    "#         )\n",
    "    \n",
    "#         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n",
    "#         attn_output_list[i] = attn_output\n",
    "        \n",
    "#     attn_output = torch.cat(attn_output_list, dim=-1)\n",
    "#     attn_output = self.o_proj(attn_output)\n",
    "\n",
    "#     if not output_attentions:\n",
    "#         attn_weights = None\n",
    "\n",
    "#     return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# LlamaFlashAttention2.forward = LlamaAttention_fast_forward\n",
    "# layer_idx = 0\n",
    "# for name, module in model.named_modules():\n",
    "#     if \"self_attn\" in name and hasattr(module, \"q_proj\"):\n",
    "#         module.layer_idx = layer_idx\n",
    "#         layer_idx += module.num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cd715d-1e47-4869-bb3f-4a10e1c7467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OffloadedCache(DynamicCache):\n",
    "    \"\"\"\n",
    "    A drop-in replacement for DynamicCache that conserves GPU memory at the expense of more CPU memory.\n",
    "    Useful for generating from models with very long context.\n",
    "\n",
    "    In addition to the default CUDA stream, where all forward() computations happen,\n",
    "    this class uses another stream, the prefetch stream, which it creates itself.\n",
    "    Since scheduling of operations on separate streams happens independently, this class uses\n",
    "    the prefetch stream to asynchronously prefetch the KV cache of layer k+1 when layer k is executing.\n",
    "    The movement of the layer k-1 cache to the CPU is handled by the default stream as a simple way to\n",
    "    ensure the eviction is scheduled after all computations on that cache are finished.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"OffloadedCache can only be used with a GPU\")\n",
    "        super().__init__()\n",
    "        self.original_device = []\n",
    "        self.prefetch_stream = torch.cuda.Stream()\n",
    "        self.beam_idx = None  # used to delay beam search operations\n",
    "\n",
    "    def prefetch_layer(self, layer_idx: int):\n",
    "        \"Starts prefetching the next layer cache\"\n",
    "        if layer_idx < len(self):\n",
    "            with torch.cuda.stream(self.prefetch_stream):\n",
    "                # Prefetch next layer tensors to GPU\n",
    "                device = self.original_device[layer_idx]\n",
    "                self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device, non_blocking=True)\n",
    "                self.value_cache[layer_idx] = self.value_cache[layer_idx].to(device, non_blocking=True)\n",
    "\n",
    "    def evict_previous_layer(self, layer_idx: int):\n",
    "        \"Moves the previous layer cache to the CPU\"\n",
    "        if len(self) > 2:\n",
    "            # We do it on the default stream so it occurs after all earlier computations on these tensors are done\n",
    "            prev_layer_idx = (layer_idx - 1) % len(self)\n",
    "            self.key_cache[prev_layer_idx] = self.key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            self.value_cache[prev_layer_idx] = self.value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n",
    "        if layer_idx < len(self):\n",
    "            # Evict the previous layer if necessary\n",
    "            torch.cuda.current_stream().synchronize()\n",
    "            self.evict_previous_layer(layer_idx)\n",
    "            # Load current layer cache to its original device if not already there\n",
    "            original_device = self.original_device[layer_idx]\n",
    "            self.prefetch_stream.synchronize()\n",
    "            key_tensor = self.key_cache[layer_idx]\n",
    "            value_tensor = self.value_cache[layer_idx]\n",
    "            # Now deal with beam search ops which were delayed\n",
    "            if self.beam_idx is not None:\n",
    "                self.beam_idx = self.beam_idx.to(original_device)\n",
    "                key_tensor = key_tensor.index_select(0, self.beam_idx)\n",
    "                value_tensor = value_tensor.index_select(0, self.beam_idx)\n",
    "            # Prefetch the next layer\n",
    "            self.prefetch_layer((layer_idx + 1) % len(self))\n",
    "            return (key_tensor, value_tensor)\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        \"\"\"Saves the beam indices and reorders the cache when the tensor is back to its device.\"\"\"\n",
    "        # We delay this operation until the tensors are back to their original\n",
    "        # device because performing torch.index_select on the CPU is very slow\n",
    "        del self.beam_idx\n",
    "        self.beam_idx = beam_idx.clone()\n",
    "\n",
    "    def move_all_to_cpu(self):\n",
    "        self.evict_previous_layer(0)\n",
    "        self.evict_previous_layer(1)\n",
    "        for i in range(len(self)):\n",
    "            print(self.key_cache[i].device, self.value_cache[i].device)\n",
    "            \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `OffloadedCache`.\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) < layer_idx:\n",
    "            raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n",
    "        elif len(self.key_cache) == layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "            self.original_device.append(key_states.device)\n",
    "            self.evict_previous_layer(layer_idx)\n",
    "        else:\n",
    "            key_tensor, value_tensor = self[layer_idx]\n",
    "            self.key_cache[layer_idx] = torch.cat([key_tensor, key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([value_tensor, value_states], dim=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    def update_cpu(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        key_states = key_states.to(\"cpu\")\n",
    "        value_states = value_states.to(\"cpu\")\n",
    "        self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "        self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    # According to https://docs.python.org/3/library/exceptions.html#NotImplementedError\n",
    "    # if a method is not supposed to be supported in a subclass we should set it to None\n",
    "    from_legacy_cache = None\n",
    "\n",
    "    to_legacy_cache = None\n",
    "\n",
    "\n",
    "\n",
    "def LlamaAttention_fast_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") :\n",
    "    if output_attentions:\n",
    "        # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n",
    "        logger.warning_once(\n",
    "            \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n",
    "            'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "        )\n",
    "        return super().forward(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "    query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    if position_embeddings is None:\n",
    "        logger.warning_once(\n",
    "            \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "            \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "            \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "            \"removed and `position_embeddings` will be mandatory.\"\n",
    "        )\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    else:\n",
    "        cos, sin = position_embeddings\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update_cpu(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "        query_states = query_states.to(\"cpu\")\n",
    "\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
    "\n",
    "    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "    is_causal = True if causal_mask is None and q_len > 1 else False\n",
    "\n",
    "    print(query_states.shape, key_states.shape, value_states.shape)\n",
    "    start_time = time.time()\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(epoch_time)\n",
    "\n",
    "    attn_output = attn_output.cuda()\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(bsz, q_len, -1)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6582c1-ade6-452b-b6f6-2ccd5d29605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Time: 4.10 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "cpu cpu\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.297576904296875\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28145289421081543\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.24117636680603027\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.27545833587646484\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2813377380371094\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2807478904724121\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28839707374572754\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2845489978790283\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28647327423095703\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2846033573150635\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28600049018859863\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2795753479003906\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2414226531982422\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.23958778381347656\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.23943233489990234\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28171348571777344\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2868783473968506\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.27515244483947754\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2865297794342041\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.27718114852905273\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2391960620880127\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.24118852615356445\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2822413444519043\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28510165214538574\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.27783799171447754\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.23545122146606445\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.242478609085083\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28124308586120605\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.274397611618042\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.24294447898864746\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.2389993667602539\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32001, 128]) torch.Size([1, 32, 32001, 128])\n",
      "0.28327226638793945\n",
      "Epoch 1/10 - Time: 12.57 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.23602294921875\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24151968955993652\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2369241714477539\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2825305461883545\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24384307861328125\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24311161041259766\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.27966856956481934\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2872297763824463\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.23899197578430176\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24273037910461426\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2772650718688965\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2873108386993408\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.23520350456237793\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2435600757598877\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2838423252105713\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.29091882705688477\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2421116828918457\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24164175987243652\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24199891090393066\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2717151641845703\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2822556495666504\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2897782325744629\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24233627319335938\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24190616607666016\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24185609817504883\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.23735904693603516\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.23647594451904297\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.28597235679626465\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.2893671989440918\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24004483222961426\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24069952964782715\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32002, 128]) torch.Size([1, 32, 32002, 128])\n",
      "0.24590253829956055\n",
      "Epoch 2/10 - Time: 13.46 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2338564395904541\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23752236366271973\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2689816951751709\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.28370141983032227\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24239850044250488\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24296331405639648\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2419123649597168\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2836644649505615\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24320626258850098\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23823118209838867\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.28090333938598633\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2438828945159912\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.242523193359375\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23884344100952148\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2850456237792969\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24260950088500977\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24320340156555176\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.27101874351501465\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.27959108352661133\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24092364311218262\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2414379119873047\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23935222625732422\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2875678539276123\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.25469136238098145\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.2440633773803711\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.27384376525878906\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.28577494621276855\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24152040481567383\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.24108552932739258\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23796439170837402\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.236069917678833\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32003, 128]) torch.Size([1, 32, 32003, 128])\n",
      "0.23691105842590332\n",
      "Epoch 3/10 - Time: 13.51 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2816019058227539\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.29378604888916016\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24942374229431152\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24046683311462402\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.28449440002441406\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2574498653411865\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2834744453430176\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.23899078369140625\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.28387880325317383\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24779295921325684\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.27713799476623535\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.285067081451416\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24640107154846191\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2518937587738037\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24322724342346191\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.27946901321411133\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24229025840759277\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.23895573616027832\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2774925231933594\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.26311564445495605\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2717618942260742\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2842142581939697\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2340843677520752\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.23624372482299805\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.282656192779541\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.23947930335998535\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24161052703857422\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.23654723167419434\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.2788264751434326\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24231266975402832\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.24890613555908203\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32004, 128]) torch.Size([1, 32, 32004, 128])\n",
      "0.28267478942871094\n",
      "Epoch 4/10 - Time: 13.94 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23886489868164062\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23816466331481934\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.28279805183410645\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24058794975280762\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.27292895317077637\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2891993522644043\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23676705360412598\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2364821434020996\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2799398899078369\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23664569854736328\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24432682991027832\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.27101993560791016\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.28289127349853516\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2395336627960205\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24227380752563477\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2774536609649658\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2408442497253418\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24017858505249023\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.28218817710876465\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.25895190238952637\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2781798839569092\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24371790885925293\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.24576497077941895\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2742652893066406\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.28528928756713867\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23507404327392578\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2341763973236084\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2818603515625\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2533423900604248\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.28931474685668945\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.23770928382873535\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32005, 128]) torch.Size([1, 32, 32005, 128])\n",
      "0.2805180549621582\n",
      "Epoch 5/10 - Time: 13.89 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24054646492004395\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23778748512268066\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23795700073242188\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2826855182647705\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23992443084716797\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23894691467285156\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.28333568572998047\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23460721969604492\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23322677612304688\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2334003448486328\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23500800132751465\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2335810661315918\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2394099235534668\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2928745746612549\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2430417537689209\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2816920280456543\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2353653907775879\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.23549270629882812\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.28188586235046387\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24475622177124023\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2769744396209717\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24509191513061523\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24918246269226074\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2791414260864258\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24405407905578613\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2439882755279541\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.2946774959564209\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.25267457962036133\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.28498172760009766\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24341344833374023\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.28479957580566406\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32006, 128]) torch.Size([1, 32, 32006, 128])\n",
      "0.24495410919189453\n",
      "Epoch 6/10 - Time: 13.87 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28606557846069336\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24732279777526855\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.27112698554992676\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28574061393737793\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2433159351348877\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24806976318359375\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28281402587890625\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.23772907257080078\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.23470163345336914\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2755258083343506\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24018335342407227\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28017759323120117\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.23922014236450195\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.23876047134399414\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2802262306213379\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.23842787742614746\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2742743492126465\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2862274646759033\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24245381355285645\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2721889019012451\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2844994068145752\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24193954467773438\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28746795654296875\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24047565460205078\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2388904094696045\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28111743927001953\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2451622486114502\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2774496078491211\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2372570037841797\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.2394394874572754\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.28585290908813477\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32007, 128]) torch.Size([1, 32, 32007, 128])\n",
      "0.24147891998291016\n",
      "Epoch 7/10 - Time: 14.00 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2806370258331299\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24012231826782227\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24133849143981934\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2823672294616699\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24431991577148438\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2958791255950928\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24219369888305664\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.28509044647216797\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24251937866210938\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.23366618156433105\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2805030345916748\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.23650836944580078\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2522468566894531\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.28200864791870117\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24246835708618164\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2790670394897461\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.23553466796875\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.23811626434326172\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2753274440765381\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2578427791595459\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2406935691833496\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2409651279449463\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2914438247680664\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24299383163452148\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.28692030906677246\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24126863479614258\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2820138931274414\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.23543477058410645\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2383277416229248\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.2864086627960205\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.24217605590820312\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32008, 128]) torch.Size([1, 32, 32008, 128])\n",
      "0.28290414810180664\n",
      "Epoch 8/10 - Time: 14.14 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24470853805541992\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.27695441246032715\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24314403533935547\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24305057525634766\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2836942672729492\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24645352363586426\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2806718349456787\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2416672706604004\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.23676323890686035\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2779858112335205\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2401418685913086\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.28114771842956543\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2435622215270996\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2851102352142334\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24396729469299316\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.28073596954345703\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24349451065063477\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.28146958351135254\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24223756790161133\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2822532653808594\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24168181419372559\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.270280122756958\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.284165620803833\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.238175630569458\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.27967166900634766\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2341327667236328\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.24734258651733398\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.28130626678466797\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.23900079727172852\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2734184265136719\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.2441561222076416\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32009, 128]) torch.Size([1, 32, 32009, 128])\n",
      "0.27451038360595703\n",
      "Epoch 9/10 - Time: 14.18 seconds\n",
      "Peak allocated bytes on 17.056775GB\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24438047409057617\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2395327091217041\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.28665876388549805\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2633540630340576\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24451971054077148\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.28145360946655273\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24098658561706543\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2743384838104248\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24425196647644043\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2395775318145752\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2875559329986572\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24716758728027344\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.28397178649902344\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24088430404663086\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.27477192878723145\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.23839235305786133\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.248582124710083\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2844102382659912\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24242687225341797\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2749807834625244\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24349236488342285\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.23792171478271484\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2785935401916504\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.23967361450195312\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24268364906311035\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.27558302879333496\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24571514129638672\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2743368148803711\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.24030423164367676\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.23993444442749023\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.2802612781524658\n",
      "torch.Size([1, 32, 1, 128]) torch.Size([1, 32, 32010, 128]) torch.Size([1, 32, 32010, 128])\n",
      "0.23813891410827637\n",
      "Epoch 10/10 - Time: 14.16 seconds\n",
      "Peak allocated bytes on 17.056775GB\n"
     ]
    }
   ],
   "source": [
    "# Manually perform inference using KV cache\n",
    "input_ids = torch.randint(0, tokenizer.vocab_size, (1, 32000)).to('cuda')\n",
    "next_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 1)).to('cuda')\n",
    "\n",
    "max_new_tokens = 1\n",
    "next_new_tokens = 10\n",
    "generated_tokens = []\n",
    "\n",
    "# Initialize past_key_values to None\n",
    "past_key_values = OffloadedCache()\n",
    "\n",
    "for epoch in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{max_new_tokens} - Time: {epoch_time:.2f} seconds\")\n",
    "        print(\n",
    "            \"Peak allocated bytes on {:4f}GB\".format(\n",
    "                torch.cuda.memory_stats(0)[\"allocated_bytes.all.peak\"] / 2**30\n",
    "            )\n",
    "        )\n",
    "\n",
    "past_key_values.move_all_to_cpu()\n",
    "\n",
    "LlamaFlashAttention2.forward = LlamaAttention_fast_forward\n",
    "for epoch in range(next_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids=next_input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{next_new_tokens} - Time: {epoch_time:.2f} seconds\")\n",
    "        print(\n",
    "            \"Peak allocated bytes on {:4f}GB\".format(\n",
    "                torch.cuda.memory_stats(0)[\"allocated_bytes.all.peak\"] / 2**30\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefbfcf-e402-4e8d-8c88-ba476306fb31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
