{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ee6126-f815-4456-85ca-459177f91cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69b450ae65541ce9025b829164569d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# Load the model\n",
    "ckpt = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(ckpt)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "# add some tokens like \"</user>\" and </s> to eos ids\n",
    "eos_token_ids += tokenizer.encode(\"</user>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</s>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b681a123-2fac-4739-b1ba-080becdb0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence length: 10033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"A quick brown fox jumps over the lazy dog. \\n\"\n",
    "# with open(\"demo/duo_attention.txt\", \"r\") as f:\n",
    "#     needle = f.read()\n",
    "needle = \"Mary's favorite number is 34251\"\n",
    "num_tokens_context = len(tokenizer.encode(context, add_special_tokens=False))\n",
    "num_repetitions = 10000 // num_tokens_context\n",
    "\n",
    "text = (\n",
    "    \"This is a very long story book: <book> \"\n",
    "    + context * int(num_repetitions * 0.75)\n",
    "    + needle\n",
    "    + context * int(num_repetitions * (1 - 0.75))\n",
    "    + \"</book>\\n Based on the content of the book, please briefly tell me about what is Mary's favorite number.\\nAnswer:\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Input sequence length: {input_ids.size(1)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629ae902-cd6b-4b4f-bd85-e14df21bd75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers.cache_utils import Cache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "class DynamicCache(Cache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Prepare a cache class and pass it to model's forward\n",
    "        >>> past_key_values = DynamicCache()\n",
    "        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n",
    "        >>> outputs.past_key_values # access cache filled with key/values from generation\n",
    "        DynamicCache()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n",
    "        sequence length.\n",
    "        \"\"\"\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n",
    "        keys and values\n",
    "        \"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n",
    "        to the number of layers in the model.\n",
    "        \"\"\"\n",
    "        return len(self.key_cache)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            # There may be skipped layers, fill them with empty lists\n",
    "            for _ in range(len(self.key_cache), layer_idx):\n",
    "                self.key_cache.append([])\n",
    "                self.value_cache.append([])\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
    "            self.key_cache[layer_idx] = key_states\n",
    "            self.value_cache[layer_idx] = value_states\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        key, value = self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "        incoming_kv_seq_len = self.key_cache[layer_idx].shape[2]\n",
    "        if incoming_kv_seq_len > self.sink_size + self.recent_size:\n",
    "            sink_key_states = self.key_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "            recent_key_states = self.key_cache[layer_idx][\n",
    "                :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "            ]\n",
    "            self.key_cache[layer_idx] = torch.cat([sink_key_states, recent_key_states], dim=-2)\n",
    "\n",
    "            sink_value_states = self.value_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "            recent_value_states = self.value_cache[layer_idx][\n",
    "                :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "            ]\n",
    "            self.value_cache[layer_idx] = torch.cat([sink_value_states, recent_value_states], dim=-2)\n",
    "\n",
    "        return key, value\n",
    "\n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        # TODO: deprecate this function in favor of `cache_position`\n",
    "        is_empty_layer = (\n",
    "            len(self.key_cache) == 0  # no cache in any layer\n",
    "            or len(self.key_cache) <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n",
    "            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
    "        )\n",
    "        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n",
    "        return layer_seq_length\n",
    "\n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format. Used for\n",
    "        backward compatibility.\"\"\"\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "        return legacy_cache\n",
    "\n",
    "    @classmethod\n",
    "    def from_legacy_cache(\n",
    "        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, num_hidden_layers: int = None\n",
    "    ) -> \"DynamicCache\":\n",
    "        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`. Used for\n",
    "        backward compatibility.\"\"\"\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(key_states, value_states, layer_idx)\n",
    "        return cache\n",
    "\n",
    "    def crop(self, max_length: int):\n",
    "        \"\"\"Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n",
    "        negative to remove `max_length` tokens. This is used in assisted decoding and contrastive search.\"\"\"\n",
    "        # In case it is negative\n",
    "        if max_length < 0:\n",
    "            max_length = self.get_seq_length() - abs(max_length)\n",
    "\n",
    "        if self.get_seq_length() <= max_length:\n",
    "            return\n",
    "\n",
    "        self._seen_tokens = max_length\n",
    "        for idx in range(len(self.key_cache)):\n",
    "            if self.key_cache[idx] != []:\n",
    "                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n",
    "                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n",
    "\n",
    "    def batch_split(\n",
    "        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n",
    "    ) -> List[\"DynamicCache\"]:\n",
    "        \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n",
    "        `_split_model_inputs()` in `generation.utils`\"\"\"\n",
    "        out = []\n",
    "        for i in range(0, full_batch_size, split_size):\n",
    "            current_split = DynamicCache()\n",
    "            current_split._seen_tokens = self._seen_tokens\n",
    "            current_split.key_cache = [tensor[i : i + split_size] for tensor in self.key_cache]\n",
    "            current_split.value_cache = [tensor[i : i + split_size] for tensor in self.value_cache]\n",
    "            out.append(current_split)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int = None) -> \"DynamicCache\":\n",
    "        \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n",
    "        `generation.utils`\"\"\"\n",
    "        cache = cls()\n",
    "        for idx in range(len(splits[0])):\n",
    "            key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n",
    "            value_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n",
    "            if key_cache != []:\n",
    "                layer_keys = torch.cat(key_cache, dim=0)\n",
    "                layer_values = torch.cat(value_cache, dim=0)\n",
    "                cache.update(layer_keys, layer_values, idx)\n",
    "        return cache\n",
    "\n",
    "    def batch_repeat_interleave(self, repeats: int):\n",
    "        \"\"\"Repeat the cache `repeats` times in the batch dimension. Used in contrastive search.\"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
    "\n",
    "    def batch_select_indices(self, indices: torch.Tensor):\n",
    "        \"\"\"Only keep the `indices` in the batch dimension of the cache. Used in contrastive search.\"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx][indices, ...]\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6582c1-ade6-452b-b6f6-2ccd5d29605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mary had a little lamb. \n",
      "Mary had a little lamb. Its fleece of white lamb. Its fleece as white as white as white as white as snow. And Mary's fleece. And Mary's little lamb. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And one. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And. And.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually perform inference using KV cache\n",
    "\n",
    "# inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n",
    "max_new_tokens = 500\n",
    "generated_tokens = []\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Initialize past_key_values to None\n",
    "past_key_values = DynamicCache()\n",
    "past_key_values.sink_size = 64\n",
    "past_key_values.recent_size = 256\n",
    "\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Logits of the last token\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "\n",
    "        # Greedy decoding: get the token with the highest probability\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        if next_token.item() in eos_token_ids:\n",
    "            break\n",
    "        generated_tokens.append(next_token.item())\n",
    "\n",
    "        # Only pass the new token for the next iteration\n",
    "        input_ids = next_token.unsqueeze(-1)\n",
    "\n",
    "# Convert generated token ids to text\n",
    "output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289402d3-2a43-41c9-892d-e4286b722d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
