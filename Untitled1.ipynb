{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ee6126-f815-4456-85ca-459177f91cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3ac19bbe2242e5afc8c46882cc5223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "ckpt = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3dd6ce-0997-4574-99a1-085cfe7c1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8)\n",
      "128\n",
      "256\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]] 0.5\n"
     ]
    }
   ],
   "source": [
    "from duo_attn.utils import load_attn_pattern, sparsify_attention_heads\n",
    "\n",
    "# Load the attention pattern\n",
    "attn_heads, sink_size, recent_size = load_attn_pattern(\n",
    "    \"attn_patterns/Llama-3-8B-Instruct-Gradient-1048k/lr=0.02-reg=0.05-ctx=1000_32000-multi_passkey10\"\n",
    ")\n",
    "\n",
    "print(attn_heads.shape)\n",
    "print(sink_size)\n",
    "print(recent_size)\n",
    "\n",
    "# Sparsify attention heads\n",
    "attn_heads, sparsity = sparsify_attention_heads(attn_heads, sparsity=0.5)\n",
    "\n",
    "print(attn_heads, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc7c131-8332-4343-9748-9fad20cdac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaRMSNorm\n",
    "from transformers.models.mistral.modeling_mistral import MistralRMSNorm\n",
    "import torch\n",
    "import flashinfer\n",
    "import types\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def flashinfer_rmsnorm_forward(self, hidden_states):\n",
    "    bsz, seq_len, hidden_size = hidden_states.size()\n",
    "    hidden_states = flashinfer.norm.rmsnorm(\n",
    "        hidden_states.view(bsz * seq_len, hidden_size),\n",
    "        self.weight,\n",
    "        eps=self.variance_epsilon,\n",
    "    )\n",
    "    return hidden_states.view(bsz, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "def enable_flashinfer_rmsnorm(model):\n",
    "    print(\"Replacing RMSNorm with Flashinfer's RMSNorm\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LlamaRMSNorm):\n",
    "            module.forward = types.MethodType(flashinfer_rmsnorm_forward, module)\n",
    "        elif isinstance(module, MistralRMSNorm):\n",
    "            module.forward = types.MethodType(flashinfer_rmsnorm_forward, module)\n",
    "    return model\n",
    "\n",
    "\n",
    "def apply_rope_inplace(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    offsets: torch.Tensor,\n",
    "    rope_scale: float,\n",
    "    rope_theta: float,\n",
    "    indptr: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    bsz, seq_len, num_heads, head_dim = q.size()\n",
    "    _, _, num_kv_heads, _ = k.size()\n",
    "    nnz = bsz * seq_len\n",
    "    q = q.view(nnz, num_heads, head_dim)\n",
    "    k = k.view(nnz, num_kv_heads, head_dim)\n",
    "    if indptr is None:\n",
    "        indptr = torch.tensor(\n",
    "            [i * seq_len for i in range(bsz + 1)], dtype=torch.int32, device=q.device\n",
    "        )\n",
    "    if offsets.numel() == 1:\n",
    "        offsets = offsets.expand(bsz).contiguous()\n",
    "    flashinfer.rope.apply_rope_inplace(\n",
    "        q,\n",
    "        k,\n",
    "        indptr,\n",
    "        offsets,\n",
    "        interleave=False,\n",
    "        rope_scale=rope_scale,\n",
    "        rope_theta=rope_theta,\n",
    "    )\n",
    "    q = q.view(bsz, seq_len, num_heads, head_dim)\n",
    "    k = k.view(bsz, seq_len, num_kv_heads, head_dim)\n",
    "    return q, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519398d8-145f-4a4b-8e8a-cda255257ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def reorder_linear_weights(\n",
    "    linear_module: torch.nn.Linear,\n",
    "    full_attention_heads: torch.Tensor,\n",
    "    repeat_num,\n",
    "    reorder_channel,\n",
    "):\n",
    "    assert reorder_channel in [\"in\", \"out\"]\n",
    "    full_attention_heads = torch.repeat_interleave(\n",
    "        full_attention_heads, repeats=repeat_num\n",
    "    ).to(linear_module.weight.device)\n",
    "    full_attn_mask = full_attention_heads > 0.5\n",
    "    if reorder_channel == \"in\":\n",
    "        weight1 = linear_module.weight.data[:, full_attn_mask]\n",
    "        weight2 = linear_module.weight.data[:, ~full_attn_mask]\n",
    "        reordered_weight = torch.cat([weight1, weight2], dim=1)\n",
    "    else:\n",
    "        weight1 = linear_module.weight.data[full_attn_mask, :]\n",
    "        weight2 = linear_module.weight.data[~full_attn_mask, :]\n",
    "        reordered_weight = torch.cat([weight1, weight2], dim=0)\n",
    "    linear_module.weight.data = reordered_weight\n",
    "    return linear_module\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def reorder_full_attn_heads(\n",
    "    full_attention_heads: torch.Tensor,\n",
    "):\n",
    "    full_attn_mask = full_attention_heads > 0.5\n",
    "    num_full_attn_heads = full_attn_mask.sum().item()\n",
    "    full_attention_heads[:num_full_attn_heads] = 1\n",
    "    full_attention_heads[num_full_attn_heads:] = 0\n",
    "    return full_attention_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c1771c-f24a-464c-8f37-dec359838938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing RMSNorm with Flashinfer's RMSNorm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    repeat_kv,\n",
    "    apply_rotary_pos_emb,\n",
    "    CausalLMOutputWithPast,\n",
    "    List,\n",
    "    Union,\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "import types\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class DuoAttentionStaticKVCache:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        full_attention_heads,\n",
    "        batch_size,\n",
    "        max_size,\n",
    "        sink_size,\n",
    "        recent_size,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = max_size\n",
    "        self.sink_size = sink_size\n",
    "        self.recent_size = recent_size\n",
    "\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.dtype = next(model.parameters()).dtype\n",
    "        self.num_layers = model.config.num_hidden_layers\n",
    "        self.num_heads = model.config.num_attention_heads\n",
    "        self.num_kv_heads = model.config.num_key_value_heads\n",
    "        self.num_kv_groups = self.num_heads // self.num_kv_heads\n",
    "        self.head_dim = model.config.hidden_size // self.num_heads\n",
    "\n",
    "        self.num_full_kv_head_list = [0] * self.num_layers\n",
    "        self.num_streaming_kv_head_list = [0] * self.num_layers\n",
    "\n",
    "        self.kv_seq_len_list = [0] * self.num_layers\n",
    "        self.streaming_kv_seq_len_list = [0] * self.num_layers\n",
    "\n",
    "        self.streaming_key_states_list = []\n",
    "        self.streaming_value_states_list = []\n",
    "        self.full_key_states_list = []\n",
    "        self.full_value_states_list = []\n",
    "\n",
    "        for idx, layer_full_attention_heads in enumerate(full_attention_heads):\n",
    "            layer_full_attention_heads = torch.tensor(layer_full_attention_heads) > 0.5\n",
    "            num_full_kv_head = layer_full_attention_heads.sum().item()\n",
    "            num_streaming_kv_head = self.num_kv_heads - num_full_kv_head\n",
    "\n",
    "            self.num_full_kv_head_list[idx] = num_full_kv_head\n",
    "            self.num_streaming_kv_head_list[idx] = num_streaming_kv_head\n",
    "\n",
    "            streaming_key_states = torch.zeros(\n",
    "                self.batch_size,\n",
    "                self.sink_size + self.recent_size,\n",
    "                num_streaming_kv_head,\n",
    "                self.head_dim,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "\n",
    "            streaming_value_states = torch.zeros(\n",
    "                self.batch_size,\n",
    "                self.sink_size + self.recent_size,\n",
    "                num_streaming_kv_head,\n",
    "                self.head_dim,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "\n",
    "            full_key_states = torch.zeros(\n",
    "                self.batch_size,\n",
    "                self.max_size,\n",
    "                num_full_kv_head,\n",
    "                self.head_dim,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "\n",
    "            full_value_states = torch.zeros(\n",
    "                self.batch_size,\n",
    "                self.max_size,\n",
    "                num_full_kv_head,\n",
    "                self.head_dim,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "\n",
    "            self.streaming_key_states_list.append(streaming_key_states)\n",
    "            self.streaming_value_states_list.append(streaming_value_states)\n",
    "            self.full_key_states_list.append(full_key_states)\n",
    "            self.full_value_states_list.append(full_value_states)\n",
    "\n",
    "    @property\n",
    "    def streaming_kv_seq_len(self):\n",
    "        return self.streaming_kv_seq_len_list[-1]\n",
    "\n",
    "    @property\n",
    "    def kv_seq_len(self):\n",
    "        return self.kv_seq_len_list[-1]\n",
    "\n",
    "    def put_full_kv(self, layer_idx, full_key_states, full_value_states):\n",
    "        incoming_kv_seq_len = full_key_states.shape[1]\n",
    "        kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "        if incoming_kv_seq_len + kv_seq_len > self.max_size:\n",
    "            raise ValueError(\n",
    "                f\"Trying to put {incoming_kv_seq_len} KVs into a cache with max size {self.max_size}, current size: {kv_seq_len}.\"\n",
    "            )\n",
    "\n",
    "        self.full_key_states_list[layer_idx][\n",
    "            :, kv_seq_len : kv_seq_len + incoming_kv_seq_len\n",
    "        ].copy_(full_key_states)\n",
    "        self.full_value_states_list[layer_idx][\n",
    "            :, kv_seq_len : kv_seq_len + incoming_kv_seq_len\n",
    "        ].copy_(full_value_states)\n",
    "\n",
    "        self.kv_seq_len_list[layer_idx] += incoming_kv_seq_len\n",
    "        return self.get_full_kv(layer_idx)\n",
    "\n",
    "    def compress_and_replace_streaming_kv(\n",
    "        self, layer_idx, streaming_key_states, streaming_value_states\n",
    "    ):\n",
    "        incoming_kv_seq_len = streaming_key_states.shape[1]\n",
    "        if incoming_kv_seq_len <= self.sink_size + self.recent_size:\n",
    "            self.streaming_key_states_list[layer_idx][\n",
    "                :,\n",
    "                :incoming_kv_seq_len,\n",
    "            ].copy_(streaming_key_states)\n",
    "            self.streaming_value_states_list[layer_idx][\n",
    "                :,\n",
    "                :incoming_kv_seq_len,\n",
    "            ].copy_(streaming_value_states)\n",
    "\n",
    "            self.streaming_kv_seq_len_list[layer_idx] = incoming_kv_seq_len\n",
    "        else:\n",
    "            sink_key_states = streaming_key_states[:, : self.sink_size]\n",
    "            recent_key_states = streaming_key_states[\n",
    "                :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len\n",
    "            ]\n",
    "            self.streaming_key_states_list[layer_idx][:, : self.sink_size].copy_(\n",
    "                sink_key_states\n",
    "            )\n",
    "            self.streaming_key_states_list[layer_idx][\n",
    "                :, self.sink_size : self.sink_size + self.recent_size\n",
    "            ].copy_(recent_key_states)\n",
    "\n",
    "            sink_value_states = streaming_value_states[:, : self.sink_size]\n",
    "            recent_value_states = streaming_value_states[\n",
    "                :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len\n",
    "            ]\n",
    "            self.streaming_value_states_list[layer_idx][:, : self.sink_size].copy_(\n",
    "                sink_value_states\n",
    "            )\n",
    "            self.streaming_value_states_list[layer_idx][\n",
    "                :, self.sink_size : self.sink_size + self.recent_size\n",
    "            ].copy_(recent_value_states)\n",
    "\n",
    "            self.streaming_kv_seq_len_list[layer_idx] = (\n",
    "                self.recent_size + self.sink_size\n",
    "            )\n",
    "\n",
    "    def put(self, layer_idx, key_states, value_states):\n",
    "        incoming_kv_seq_len = key_states.shape[1]\n",
    "        kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "        streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "        if incoming_kv_seq_len + kv_seq_len > self.max_size:\n",
    "            raise ValueError(\n",
    "                f\"Trying to put {incoming_kv_seq_len} KVs into a cache with max size {self.max_size}, current size: {kv_seq_len}.\"\n",
    "            )\n",
    "        if (\n",
    "            incoming_kv_seq_len + streaming_kv_seq_len\n",
    "            > self.sink_size + self.recent_size + self.prefilling_chunk_size\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Trying to put {incoming_kv_seq_len} KVs into a cache with sink size {self.sink_size}, recent size {self.recent_size}, and prefilling chunk size {self.prefilling_chunk_size}, current size: {streaming_kv_seq_len}.\"\n",
    "            )\n",
    "\n",
    "        (\n",
    "            full_key_states,\n",
    "            full_value_states,\n",
    "            streaming_key_states,\n",
    "            streaming_value_states,\n",
    "        ) = self.split_kv(layer_idx, key_states, value_states)\n",
    "\n",
    "        self.full_key_states_list[layer_idx][\n",
    "            :, kv_seq_len : kv_seq_len + incoming_kv_seq_len\n",
    "        ].copy_(full_key_states)\n",
    "        self.full_value_states_list[layer_idx][\n",
    "            :, kv_seq_len : kv_seq_len + incoming_kv_seq_len\n",
    "        ].copy_(full_value_states)\n",
    "\n",
    "        self.streaming_key_states_list[layer_idx][\n",
    "            :,\n",
    "            streaming_kv_seq_len : streaming_kv_seq_len + incoming_kv_seq_len,\n",
    "        ].copy_(streaming_key_states)\n",
    "        self.streaming_value_states_list[layer_idx][\n",
    "            :,\n",
    "            streaming_kv_seq_len : streaming_kv_seq_len + incoming_kv_seq_len,\n",
    "        ].copy_(streaming_value_states)\n",
    "\n",
    "        self.update_seq_len(layer_idx, incoming_kv_seq_len)\n",
    "\n",
    "        return self.get(layer_idx)\n",
    "\n",
    "    def update_seq_len(self, layer_idx, incoming_kv_seq_len):\n",
    "        self.kv_seq_len_list[layer_idx] += incoming_kv_seq_len\n",
    "        self.streaming_kv_seq_len_list[layer_idx] += incoming_kv_seq_len\n",
    "\n",
    "    def get_full_kv(self, layer_idx):\n",
    "        kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "        return (\n",
    "            self.full_key_states_list[layer_idx][:, :kv_seq_len],\n",
    "            self.full_value_states_list[layer_idx][:, :kv_seq_len],\n",
    "        )\n",
    "\n",
    "    def get_streaming_kv(self, layer_idx):\n",
    "        streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "        return (\n",
    "            self.streaming_key_states_list[layer_idx][:, :streaming_kv_seq_len],\n",
    "            self.streaming_value_states_list[layer_idx][:, :streaming_kv_seq_len],\n",
    "        )\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "        streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "        return (\n",
    "            self.full_key_states_list[layer_idx][:, :kv_seq_len],\n",
    "            self.full_value_states_list[layer_idx][:, :kv_seq_len],\n",
    "            self.streaming_key_states_list[layer_idx][:, :streaming_kv_seq_len],\n",
    "            self.streaming_value_states_list[layer_idx][:, :streaming_kv_seq_len],\n",
    "        )\n",
    "\n",
    "    def get_unsliced(self, layer_idx):\n",
    "        kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "        streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "        return (\n",
    "            kv_seq_len,\n",
    "            self.full_key_states_list[layer_idx],\n",
    "            self.full_value_states_list[layer_idx],\n",
    "            streaming_kv_seq_len,\n",
    "            self.streaming_key_states_list[layer_idx],\n",
    "            self.streaming_value_states_list[layer_idx],\n",
    "        )\n",
    "\n",
    "    def split_kv(self, layer_idx, key_states, value_states):\n",
    "        num_full_kv_head = self.num_full_kv_head_list[layer_idx]\n",
    "        full_key_states = key_states[:, :, :num_full_kv_head, :]\n",
    "        full_value_states = value_states[:, :, :num_full_kv_head, :]\n",
    "        streaming_key_states = key_states[:, :, num_full_kv_head:, :]\n",
    "        streaming_value_states = value_states[:, :, num_full_kv_head:, :]\n",
    "        return (\n",
    "            full_key_states,\n",
    "            full_value_states,\n",
    "            streaming_key_states,\n",
    "            streaming_value_states,\n",
    "        )\n",
    "\n",
    "    def compress(self, layer_idx):\n",
    "        streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "        if streaming_kv_seq_len <= self.recent_size + self.sink_size:\n",
    "            return\n",
    "        recent_key_states = self.streaming_key_states_list[layer_idx][\n",
    "            :, streaming_kv_seq_len - self.recent_size : streaming_kv_seq_len\n",
    "        ].clone()\n",
    "        self.streaming_key_states_list[layer_idx][\n",
    "            :, self.sink_size : self.sink_size + self.recent_size\n",
    "        ].copy_(recent_key_states)\n",
    "\n",
    "        recent_value_states = self.streaming_value_states_list[layer_idx][\n",
    "            :, streaming_kv_seq_len - self.recent_size : streaming_kv_seq_len\n",
    "        ].clone()\n",
    "        self.streaming_value_states_list[layer_idx][\n",
    "            :, self.sink_size : self.sink_size + self.recent_size\n",
    "        ].copy_(recent_value_states)\n",
    "\n",
    "        self.streaming_kv_seq_len_list[layer_idx] = self.recent_size + self.sink_size\n",
    "\n",
    "    def clear(self):\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            self.kv_seq_len_list[layer_idx] = 0\n",
    "            self.streaming_kv_seq_len_list[layer_idx] = 0\n",
    "\n",
    "    def evict_last(self, num_tokens):\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            kv_seq_len = self.kv_seq_len_list[layer_idx]\n",
    "            streaming_kv_seq_len = self.streaming_kv_seq_len_list[layer_idx]\n",
    "            self.kv_seq_len_list[layer_idx] = max(0, kv_seq_len - num_tokens)\n",
    "            self.streaming_kv_seq_len_list[layer_idx] = max(\n",
    "                0, streaming_kv_seq_len - num_tokens\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def memory_usage(self):\n",
    "        memory_usage = 0\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            memory_usage += self.full_key_states_list[layer_idx].element_size() * (\n",
    "                self.full_key_states_list[layer_idx].numel()\n",
    "            )\n",
    "            memory_usage += self.full_value_states_list[layer_idx].element_size() * (\n",
    "                self.full_value_states_list[layer_idx].numel()\n",
    "            )\n",
    "            memory_usage += self.streaming_key_states_list[layer_idx].element_size() * (\n",
    "                self.streaming_key_states_list[layer_idx].numel()\n",
    "            )\n",
    "            memory_usage += self.streaming_value_states_list[\n",
    "                layer_idx\n",
    "            ].element_size() * (self.streaming_value_states_list[layer_idx].numel())\n",
    "        return memory_usage\n",
    "\n",
    "\n",
    "\n",
    "def duo_attn_static_kv_cache_llama_model_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[DuoAttentionStaticKVCache] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "    output_attentions = (\n",
    "        output_attentions\n",
    "        if output_attentions is not None\n",
    "        else self.config.output_attentions\n",
    "    )\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states\n",
    "        if output_hidden_states is not None\n",
    "        else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "    return_dict = (\n",
    "        return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    )\n",
    "\n",
    "    # retrieve input_ids and inputs_embeds\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\n",
    "            \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "        )\n",
    "    elif input_ids is not None:\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "    elif inputs_embeds is not None:\n",
    "        batch_size, seq_length, _ = inputs_embeds.shape\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "        \n",
    "    ###########################################\n",
    "    seq_length_with_past = seq_length\n",
    "    past_key_values_length = 0\n",
    "\n",
    "    if past_key_values is not None:\n",
    "        past_key_values_length = past_key_values.kv_seq_len\n",
    "        seq_length_with_past = seq_length_with_past + past_key_values_length\n",
    "    ###########################################\n",
    "\n",
    "    if position_ids is None:\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        position_ids = torch.arange(\n",
    "            past_key_values_length,\n",
    "            seq_length + past_key_values_length,\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "    else:\n",
    "        position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "    # embed positions\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones(\n",
    "            (batch_size, seq_length_with_past),\n",
    "            dtype=torch.bool,\n",
    "            device=inputs_embeds.device,\n",
    "        )\n",
    "        padding_mask = None\n",
    "    else:\n",
    "        if 0 in attention_mask:\n",
    "            padding_mask = attention_mask\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "    attention_mask = self._prepare_decoder_attention_mask(\n",
    "        attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
    "    )\n",
    "\n",
    "    hidden_states = inputs_embeds\n",
    "\n",
    "    # decoder layers\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    all_self_attns = () if output_attentions else None\n",
    "\n",
    "    for idx, decoder_layer in enumerate(self.layers):\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        layer_outputs = decoder_layer(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            kv_cache=past_key_values,\n",
    "            layer_idx=idx,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            padding_mask=padding_mask,\n",
    "        )\n",
    "\n",
    "        hidden_states = layer_outputs[0]\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "    hidden_states = self.norm(hidden_states)\n",
    "\n",
    "    # add hidden states from the last decoder layer\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states += (hidden_states,)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=past_key_values,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attns,\n",
    "    )\n",
    "\n",
    "def duo_attn_static_kv_cache_llama_decoder_layer_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    kv_cache: Optional[DuoAttentionStaticKVCache] = None,\n",
    "    layer_idx: int = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    padding_mask: Optional[torch.LongTensor] = None,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "    residual = hidden_states\n",
    "\n",
    "    hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights = self.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        kv_cache=kv_cache,\n",
    "        layer_idx=layer_idx,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        padding_mask=padding_mask,\n",
    "    )\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "    hidden_states = self.mlp(hidden_states)\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def duo_attn_static_kv_cache_llama_for_causal_lm_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[DuoAttentionStaticKVCache] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "    output_attentions = (\n",
    "        output_attentions\n",
    "        if output_attentions is not None\n",
    "        else self.config.output_attentions\n",
    "    )\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states\n",
    "        if output_hidden_states is not None\n",
    "        else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = (\n",
    "        return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    )\n",
    "\n",
    "    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "\n",
    "    if self.training:\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "    else:\n",
    "        logits = self.lm_head(hidden_states[:, -1:, :])\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        past_key_values=outputs.past_key_values,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "def llama_duo_attention_forward_one_way_reordered_static(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    kv_cache: Optional[DuoAttentionStaticKVCache] = None,\n",
    "    layer_idx: int = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "    value_states = value_states.view(\n",
    "        bsz, q_len, self.num_key_value_heads, self.head_dim\n",
    "    )\n",
    "\n",
    "    kv_seq_len = q_len\n",
    "    if kv_cache is not None:\n",
    "        kv_seq_len += kv_cache.kv_seq_len\n",
    "\n",
    "    # Replace the Huggingface's apply rotory pos emb with FlashInfer's rope\n",
    "\n",
    "    # cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    # query_states, key_states = apply_rotary_pos_emb(\n",
    "    #     query_states,\n",
    "    #     key_states,\n",
    "    #     cos,\n",
    "    #     sin,\n",
    "    #     unsqueeze_dim=2,  # unsqueeze_dim=2 for the flash attention\n",
    "    # )\n",
    "    rope_scale = 1.0\n",
    "    if self.config.rope_scaling is not None:\n",
    "        rope_scale = self.config.rope_scaling.get(\"factor\", 1.0)\n",
    "    apply_rope_inplace(\n",
    "        query_states, key_states, position_ids[:, 0], rope_scale, self.rope_theta\n",
    "    )\n",
    "\n",
    "def enable_duo_attention_static_kv_cache_for_llama(model: LlamaForCausalLM):\n",
    "    \n",
    "    model.model.forward = types.MethodType(\n",
    "        duo_attn_static_kv_cache_llama_model_forward, model.model\n",
    "    )\n",
    "    for idx in range(len(model.model.layers)):\n",
    "        model.model.layers[idx].forward = types.MethodType(\n",
    "            duo_attn_static_kv_cache_llama_decoder_layer_forward,\n",
    "            model.model.layers[idx],\n",
    "        )\n",
    "    model.forward = types.MethodType(\n",
    "        duo_attn_static_kv_cache_llama_for_causal_lm_forward, model\n",
    "    )\n",
    "\n",
    "def enable_llama_duo_attention_eval(\n",
    "    model: LlamaForCausalLM,\n",
    "    full_attention_heads,\n",
    "    sink_size,\n",
    "    recent_size,\n",
    "):\n",
    "    enable_duo_attention_static_kv_cache_for_llama(model)\n",
    "    enable_flashinfer_rmsnorm(model)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        module = layer.self_attn\n",
    "        layer_full_attention_heads = torch.tensor(\n",
    "            full_attention_heads[idx], device=device, dtype=dtype\n",
    "        )\n",
    "        module.forward = types.MethodType(\n",
    "            llama_duo_attention_forward_one_way_reordered_static, module\n",
    "        )\n",
    "        module.q_proj = reorder_linear_weights(\n",
    "            module.q_proj,\n",
    "            layer_full_attention_heads,\n",
    "            module.num_key_value_groups * module.head_dim,\n",
    "            \"out\",\n",
    "        )\n",
    "        module.k_proj = reorder_linear_weights(\n",
    "            module.k_proj,\n",
    "            layer_full_attention_heads,\n",
    "            module.head_dim,\n",
    "            \"out\",\n",
    "        )\n",
    "        module.v_proj = reorder_linear_weights(\n",
    "            module.v_proj,\n",
    "            layer_full_attention_heads,\n",
    "            module.head_dim,\n",
    "            \"out\",\n",
    "        )\n",
    "        module.o_proj = reorder_linear_weights(\n",
    "            module.o_proj,\n",
    "            layer_full_attention_heads,\n",
    "            module.num_key_value_groups * module.head_dim,\n",
    "            \"in\",\n",
    "        )\n",
    "\n",
    "enable_llama_duo_attention_eval(\n",
    "    model,\n",
    "    attn_heads,\n",
    "    sink_size=64,\n",
    "    recent_size=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a543c4-370e-4d7e-9045-ce2eb17885c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence length: 462\n",
      "\n",
      "This is a very long story book: <book> A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "Title: DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads\n",
      "Abstract: Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks, referred to as Streaming Heads, do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU.A quick brown fox jumps over the lazy dog. \n",
      "A quick brown fox jumps over the lazy dog. \n",
      "</book>\n",
      " Based on the content of the book, please briefly tell me about DuoAttention.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-filling (0/462):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 462])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "duo_attn_static_kv_cache_llama_for_causal_lm_forward() got an unexpected keyword argument 'kv_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[1;32m     46\u001b[0m used_mem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\n\u001b[0;32m---> 47\u001b[0m test_with_chunked_prefilling(\u001b[38;5;241m32000\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak memory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mused_mem\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtest_with_chunked_prefilling\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m chunk_input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, i : i \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunk_input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     37\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mchunk_input_ids,\n\u001b[1;32m     38\u001b[0m     kv_cache\u001b[38;5;241m=\u001b[39mkv_cache,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: duo_attn_static_kv_cache_llama_for_causal_lm_forward() got an unexpected keyword argument 'kv_cache'"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"A quick brown fox jumps over the lazy dog. \\n\"\n",
    "with open(\"demo/duo_attention.txt\", \"r\") as f:\n",
    "    needle = f.read()\n",
    "\n",
    "insertion_point = 0.75\n",
    "num_tokens_context = len(tokenizer.encode(context, add_special_tokens=False))\n",
    "num_repetitions = 100 // num_tokens_context\n",
    "text = (\n",
    "    \"This is a very long story book: <book> \"\n",
    "    + context * int(num_repetitions * insertion_point)\n",
    "    + needle\n",
    "    + context * int(num_repetitions * (1 - insertion_point))\n",
    "    + \"</book>\\n Based on the content of the book, please briefly tell me about DuoAttention.\\nAnswer:\"\n",
    ")\n",
    "\n",
    "\n",
    "def test_with_chunked_prefilling(chunk_size=32000):\n",
    "    \n",
    "    kv_cache = DuoAttentionStaticKVCache(\n",
    "        model=model,\n",
    "        full_attention_heads=attn_heads,\n",
    "        batch_size=1,\n",
    "        max_size=input_ids.size(1) + 550,\n",
    "        sink_size=sink_size,\n",
    "        recent_size=recent_size,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(\n",
    "            range(0, input_ids.size(1), chunk_size),\n",
    "            desc=f\"Pre-filling ({0}/{input_ids.size(1)})\",\n",
    "        )\n",
    "        for i in pbar:\n",
    "            chunk_input_ids = input_ids[:, i : i + chunk_size]\n",
    "            print(chunk_input_ids.shape)\n",
    "            logits = model(\n",
    "                input_ids=chunk_input_ids,\n",
    "                kv_cache=kv_cache,\n",
    "            )\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Input sequence length: {input_ids.size(1)}\\n\")\n",
    "print(text)\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "used_mem = torch.cuda.max_memory_allocated()\n",
    "test_with_chunked_prefilling(32000)\n",
    "print(f\"Peak memory: {used_mem / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6582c1-ade6-452b-b6f6-2ccd5d29605c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
