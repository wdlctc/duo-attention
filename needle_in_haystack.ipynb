{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273de29e-3843-4cf1-9e77-b18d3ae935f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cf65db-0538-4e08-9a7a-3459c0febcbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: ''. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 468\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# asyncio.run(self.run_test())\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_test(args)\n\u001b[0;32m--> 468\u001b[0m ht \u001b[38;5;241m=\u001b[39m LLMNeedleHaystackTester()\n",
      "Cell \u001b[0;32mIn[8], line 140\u001b[0m, in \u001b[0;36mLLMNeedleHaystackTester.__init__\u001b[0;34m(self, needle, haystack_dir, retrieval_question, results_version, context_lengths_min, context_lengths_max, context_lengths_num_intervals, context_lengths, document_depth_percent_min, document_depth_percent_max, document_depth_percent_intervals, document_depth_percents, document_depth_percent_interval_type, model_provider, model_name, model_name_suffix, num_concurrent_requests, save_results, save_contexts, final_context_length_buffer, seconds_to_sleep_between_completions, print_ongoing_status, attn_load_dir, sparsity, simulation_length)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_depth_percent_interval_type must be either None, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md like your own distribution give a list of ints in via document_depth_percent_intervals\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:857\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    859\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:689\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    688\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 689\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    692\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    693\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    694\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    695\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    696\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    697\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    698\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    699\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    700\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    701\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    702\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    703\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    704\u001b[0m )\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: ''. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "\n",
    "class LLMNeedleHaystackTester:\n",
    "    \"\"\"\n",
    "    This class is used to test the LLM Needle Haystack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        needle=\"\\n\\nRemember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\\n\",\n",
    "        haystack_dir=\"PaulGrahamEssays\",\n",
    "        retrieval_question=\"what is the best thing to do in San Francisco?\\n\\nAnswer: The best thing to do in San Francisco is\",\n",
    "        results_version=1,\n",
    "        context_lengths_min=1000,\n",
    "        context_lengths_max=1048000,\n",
    "        context_lengths_num_intervals=40,\n",
    "        context_lengths=None,\n",
    "        document_depth_percent_min=0,\n",
    "        document_depth_percent_max=100,\n",
    "        document_depth_percent_intervals=10,\n",
    "        document_depth_percents=None,\n",
    "        document_depth_percent_interval_type=\"linear\",\n",
    "        model_provider=\"LLaMa\",\n",
    "        model_name=\"\",\n",
    "        model_name_suffix=None,\n",
    "        num_concurrent_requests=1,\n",
    "        save_results=True,\n",
    "        save_contexts=True,\n",
    "        final_context_length_buffer=200,\n",
    "        seconds_to_sleep_between_completions=None,\n",
    "        print_ongoing_status=True,\n",
    "        attn_load_dir=None,\n",
    "        sparsity=0.5,\n",
    "        simulation_length=50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param needle: The needle to be found in the haystack. Default is None.\n",
    "        :param haystack_dir: The directory of text files to use as background context (or a haystack) in which the needle is to be found. Default is Paul Graham Essays.\n",
    "        :param retrieval_question: The question which with to prompt the model to do the retrieval.\n",
    "        :param results_version: In case you would like to try the same combination of model, context length, and depth % multiple times, change the results version other than 1\n",
    "        :param num_concurrent_requests: Due to volume, this object is set up to run concurrent requests, default = 1. Be careful of rate limits.\n",
    "        :param save_results: Whether or not you would like to save your contexts to file. Warning: These will get long! Default = True\n",
    "        :param save_contexts: Whether or not you would like to save your contexts to file. Warning: These will get long! Default is True.\n",
    "        :param final_context_length_buffer: The amount of cushion you'd like to leave off the input context to allow for the output context. Default 200 tokens\n",
    "        :param context_lengths_min: The minimum length of the context. Default is 1000.\n",
    "        :param context_lengths_max: The maximum length of the context. Default is 200000.\n",
    "        :param context_lengths_num_intervals: The number of intervals for the context length. Default is 35.\n",
    "        :param context_lengths: The lengths of the context. Default is None.\n",
    "        :param document_depth_percent_min: The minimum depth percent of the document. Default is 0.\n",
    "        :param document_depth_percent_max: The maximum depth percent of the document. Default is 100.\n",
    "        :param document_depth_percent_intervals: The number of intervals for the document depth percent. Default is 35.\n",
    "        :param document_depth_percents: The depth percentages of the document. Default is None.\n",
    "        :param document_depth_percent_interval_type: The type of interval for the document depth percent. Must be either 'linear' or 'sigmoid'. Default is 'linear'.\n",
    "        :param model_name: The name of the model. Default is 'gpt-4-1106-preview'.\n",
    "        :param seconds_to_sleep_between_completions: The number of seconds to sleep between completions. Default is None.\n",
    "        :param print_ongoing_status: Whether or not to print the ongoing status. Default is True.\n",
    "        \"\"\"\n",
    "        if not needle or not haystack_dir or not retrieval_question:\n",
    "            raise ValueError(\n",
    "                \"Needle, haystack, and retrieval_question must be provided.\"\n",
    "            )\n",
    "\n",
    "        self.needle = needle\n",
    "        self.haystack_dir = haystack_dir\n",
    "        self.retrieval_question = retrieval_question\n",
    "        self.results_version = results_version\n",
    "        self.num_concurrent_requests = num_concurrent_requests\n",
    "        self.save_results = save_results\n",
    "        self.final_context_length_buffer = final_context_length_buffer\n",
    "        self.save_contexts = save_contexts\n",
    "        self.seconds_to_sleep_between_completions = seconds_to_sleep_between_completions\n",
    "        self.print_ongoing_status = print_ongoing_status\n",
    "        self.model_provider = model_provider\n",
    "        self.testing_results = []\n",
    "\n",
    "        if \"/\" in model_name:\n",
    "            self.model_version = model_name.split(\"/\")[-1]\n",
    "        else:\n",
    "            self.model_version = model_name\n",
    "        if model_name_suffix is not None:\n",
    "            self.model_version += \"_\" + model_name_suffix\n",
    "\n",
    "        if context_lengths is None:\n",
    "            if (\n",
    "                context_lengths_min is None\n",
    "                or context_lengths_max is None\n",
    "                or context_lengths_num_intervals is None\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Either context_lengths_min, context_lengths_max, context_lengths_intervals need to be filled out OR the context_lengths_list needs to be supplied.\"\n",
    "                )\n",
    "            else:\n",
    "                self.context_lengths = np.round(\n",
    "                    np.linspace(\n",
    "                        context_lengths_min,\n",
    "                        context_lengths_max,\n",
    "                        num=context_lengths_num_intervals,\n",
    "                        endpoint=True,\n",
    "                    )\n",
    "                ).astype(int)\n",
    "        else:\n",
    "            self.context_lengths = context_lengths\n",
    "\n",
    "        if document_depth_percents is None:\n",
    "            if (\n",
    "                document_depth_percent_min is None\n",
    "                or document_depth_percent_max is None\n",
    "                or document_depth_percent_intervals is None\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Either document_depth_percent_min, document_depth_percent_max, document_depth_percent_intervals need to be filled out OR the document_depth_percents needs to be supplied.\"\n",
    "                )\n",
    "            else:\n",
    "                if document_depth_percent_interval_type == \"linear\":\n",
    "                    self.document_depth_percents = np.round(\n",
    "                        np.linspace(\n",
    "                            document_depth_percent_min,\n",
    "                            document_depth_percent_max,\n",
    "                            num=document_depth_percent_intervals,\n",
    "                            endpoint=True,\n",
    "                        )\n",
    "                    ).astype(int)\n",
    "                elif document_depth_percent_interval_type == \"sigmoid\":\n",
    "                    self.document_depth_percents = [\n",
    "                        self.logistic(x)\n",
    "                        for x in np.linspace(\n",
    "                            document_depth_percent_min,\n",
    "                            document_depth_percent_max,\n",
    "                            document_depth_percent_intervals,\n",
    "                        )\n",
    "                    ]\n",
    "        else:\n",
    "            self.document_depth_percents = document_depth_percents\n",
    "\n",
    "        if document_depth_percent_interval_type not in [None, \"linear\", \"sigmoid\"]:\n",
    "            raise ValueError(\n",
    "                \"document_depth_percent_interval_type must be either None, 'linear' or 'sigmoid'. If you'd like your own distribution give a list of ints in via document_depth_percent_intervals\"\n",
    "            )\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.enc = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "        self.eos_token_ids = self.generation_config.eos_token_id\n",
    "        if not isinstance(self.eos_token_ids, list):\n",
    "            self.eos_token_ids = [self.eos_token_ids]\n",
    "\n",
    "        if self.enc.pad_token_id is None:\n",
    "            if self.enc.eos_token_id is not None:\n",
    "                self.enc.pad_token_id = self.enc.eos_token_id\n",
    "            else:\n",
    "                self.enc.pad_token_id = 0\n",
    "        print(\"Loading from %s\" % model_name)\n",
    "\n",
    "        self.model_to_test = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"eager\",\n",
    "        ).eval()\n",
    "\n",
    "        print(f\"attn_load_dir: {attn_load_dir}\")\n",
    "\n",
    "        if attn_load_dir is not None:\n",
    "            print(\n",
    "                f\"Loading attention pattern from {attn_load_dir} with sparsity {sparsity}\"\n",
    "            )\n",
    "            full_attention_heads, sink_size, recent_size = load_attn_pattern(\n",
    "                attn_load_dir\n",
    "            )\n",
    "            if args.sink_size is not None:\n",
    "                sink_size = args.sink_size\n",
    "            if args.recent_size is not None:\n",
    "                recent_size = args.recent_size\n",
    "            full_attention_heads, sparsity = sparsify_attention_heads(\n",
    "                full_attention_heads, None, sparsity\n",
    "            )\n",
    "            enable_duo_attention_eval(\n",
    "                self.model_to_test,\n",
    "                full_attention_heads,\n",
    "                sink_size,\n",
    "                recent_size,\n",
    "            )\n",
    "\n",
    "        # list all usable GPU devices using torch\n",
    "        device_list = [i for i in range(torch.cuda.device_count())]\n",
    "        self.model_to_test = to_device(self.model_to_test, device_list, enable_tp=True)\n",
    "\n",
    "        self.model_to_test_description = model_name\n",
    "\n",
    "        self.evaluation_model = None\n",
    "        self.debug = \"debug\"\n",
    "        self.simulation_length = simulation_length\n",
    "        model_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "    def logistic(self, x, L=100, x0=50, k=0.1):\n",
    "        if x == 0:\n",
    "            return 0\n",
    "        if x == 100:\n",
    "            return 100\n",
    "        return np.round(L / (1 + np.exp(-k * (x - x0))), 3)\n",
    "\n",
    "    def bound_evaluate_and_log(self, *args):\n",
    "        self.evaluate_and_log(*args)\n",
    "\n",
    "    def run_test(self, args):\n",
    "        # Run through each iteration of context_lengths and depths\n",
    "        tasks = []\n",
    "        for context_length in self.context_lengths:\n",
    "            if context_length < args.s_len or context_length > args.e_len:\n",
    "                continue\n",
    "            for depth_percent in self.document_depth_percents:\n",
    "                task = self.bound_evaluate_and_log(context_length, depth_percent)\n",
    "\n",
    "    def generate_prompt(self, context):\n",
    "        test_format = f\"<|im_start|> This is a very long story book: <book> {context} </book>.\\n\\nQuestion: Based on the content of the book, {self.retrieval_question}\"\n",
    "        return test_format\n",
    "\n",
    "    def evaluate_and_log(self, context_length, depth_percent):\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if self.save_results:\n",
    "            if self.result_exists(context_length, depth_percent):\n",
    "                print(\"result exists, skipping\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"result does not exist, testing\")\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = self.generate_context(context_length, depth_percent)\n",
    "\n",
    "        # Prepare your message to send to the model you're going to evaluate\n",
    "        prompt = self.generate_prompt(context)\n",
    "\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        # Simulate multiround conversation\n",
    "        prompt = self.enc(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        prompt_input_ids = prompt[\"input_ids\"].to(self.model_to_test.device)\n",
    "\n",
    "        simulation_start_idx = prompt_input_ids.size(1) - self.simulation_length\n",
    "\n",
    "        question_input_ids = prompt_input_ids[:, simulation_start_idx:]\n",
    "        prompt_input_ids = prompt_input_ids[:, :simulation_start_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.args.prefilling_chunk_size is not None:\n",
    "                past_key_values = None\n",
    "                for i in range(\n",
    "                    0, prompt_input_ids.size(1), self.args.prefilling_chunk_size\n",
    "                ):\n",
    "                    chunk = prompt_input_ids[:, i : i + self.args.prefilling_chunk_size]\n",
    "                    output = self.model_to_test(\n",
    "                        input_ids=chunk,\n",
    "                        past_key_values=past_key_values,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    past_key_values = output.past_key_values\n",
    "            else:\n",
    "                output = self.model_to_test(\n",
    "                    input_ids=prompt_input_ids, past_key_values=None, use_cache=True\n",
    "                )\n",
    "                past_key_values = output.past_key_values\n",
    "\n",
    "            for input_id in question_input_ids[0]:\n",
    "                output = self.model_to_test(\n",
    "                    input_ids=input_id.unsqueeze(0).unsqueeze(0),\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                past_key_values = output.past_key_values\n",
    "\n",
    "            pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "            generated_content = [pred_token_idx.item()]\n",
    "            for _ in range(50):\n",
    "                outputs = self.model_to_test(\n",
    "                    input_ids=pred_token_idx,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "\n",
    "                past_key_values = outputs.past_key_values\n",
    "                pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "                generated_content += [pred_token_idx.item()]\n",
    "                if pred_token_idx.item() in self.eos_token_ids:\n",
    "                    break\n",
    "\n",
    "        response = self.enc.decode(generated_content, skip_special_tokens=True).strip()\n",
    "\n",
    "        test_end_time = time.time()\n",
    "        test_elapsed_time = test_end_time - test_start_time\n",
    "        score = scorer.score(self.needle, response)[\"rouge1\"].fmeasure * 10\n",
    "\n",
    "        results = {\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            \"model\": self.model_to_test_description,\n",
    "            \"context_length\": int(context_length),\n",
    "            \"depth_percent\": float(depth_percent),\n",
    "            \"version\": self.results_version,\n",
    "            \"needle\": self.needle,\n",
    "            \"model_response\": response,\n",
    "            \"score\": score,\n",
    "            \"test_duration_seconds\": test_elapsed_time,\n",
    "            \"test_timestamp_utc\": datetime.now(timezone.utc).strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S%z\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.testing_results.append(results)\n",
    "\n",
    "        if self.print_ongoing_status:\n",
    "            print(f\"-- Test Summary -- \")\n",
    "            print(f\"Duration: {test_elapsed_time:.1f} seconds\")\n",
    "            print(f\"Context: {context_length} tokens\")\n",
    "            print(f\"Depth: {depth_percent}%\")\n",
    "            print(f\"Score: {score}\")\n",
    "            print(f\"Response: {response}\\n\")\n",
    "\n",
    "        context_file_location = f'{self.model_version.replace(\".\", \"_\")}_len_{context_length}_depth_{int(depth_percent*100)}'\n",
    "\n",
    "        if self.save_contexts:\n",
    "            results[\"file_name\"] = context_file_location\n",
    "\n",
    "            # Save the context to file for retesting\n",
    "            if not os.path.exists(\"contexts\"):\n",
    "                os.makedirs(\"contexts\")\n",
    "\n",
    "            if not os.path.exists(f\"contexts/{self.model_version}\"):\n",
    "                os.makedirs(f\"contexts/{self.model_version}\")\n",
    "\n",
    "            with open(\n",
    "                f\"contexts/{self.model_version}/{context_file_location}_context.txt\",\n",
    "                \"w\",\n",
    "                encoding=\"utf-8\",\n",
    "            ) as f:\n",
    "                f.write(context)\n",
    "\n",
    "        if self.save_results:\n",
    "            # Save the context to file for retesting\n",
    "            if not os.path.exists(\"results\"):\n",
    "                os.makedirs(\"results\")\n",
    "\n",
    "            if not os.path.exists(f\"results/{self.model_version}\"):\n",
    "                os.makedirs(f\"results/{self.model_version}\")\n",
    "\n",
    "            # Save the result to file for retesting\n",
    "            p = f\"results/{self.model_version}/{context_file_location}_results.json\"\n",
    "            print(\"Writing at %s\" % p)\n",
    "            with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "    def result_exists(self, context_length, depth_percent):\n",
    "        \"\"\"\n",
    "        Checks to see if a result has already been evaluated or not\n",
    "        \"\"\"\n",
    "\n",
    "        results_dir = \"results/\" + self.model_version\n",
    "        print(\"Searching existing results at %s\" % results_dir)\n",
    "        if not os.path.exists(results_dir):\n",
    "            return False\n",
    "        for filename in os.listdir(results_dir):\n",
    "            if filename.endswith(\".json\"):\n",
    "                with open(os.path.join(results_dir, filename), \"r\") as f:\n",
    "                    result = json.load(f)\n",
    "                    context_length_met = result[\"context_length\"] == context_length\n",
    "                    depth_percent_met = result[\"depth_percent\"] == depth_percent\n",
    "                    version_met = result.get(\"version\", 1) == self.results_version\n",
    "                    model_met = result[\"model\"] == self.model_name\n",
    "                    # import ipdb; ipdb.set_trace()\n",
    "                    if (\n",
    "                        context_length_met\n",
    "                        and depth_percent_met\n",
    "                        and version_met\n",
    "                        and model_met\n",
    "                    ):\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def generate_context(self, context_length, depth_percent):\n",
    "        # Load up tiktoken so we navigate tokens more easily\n",
    "\n",
    "        # Get your Paul Graham files loaded into a string\n",
    "        context = self.read_context_files()\n",
    "\n",
    "        # Truncate the Paul Graham essays to the context length you desire\n",
    "        context = self.encode_and_trim(context, context_length)\n",
    "\n",
    "        # Insert your random statement according to your depth percent\n",
    "        context = self.insert_needle(context, depth_percent, context_length)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def encode_text_to_tokens(self, text):\n",
    "        return self.enc.encode(text, add_special_tokens=False)\n",
    "\n",
    "    def insert_needle(self, context, depth_percent, context_length):\n",
    "        tokens_needle = self.encode_text_to_tokens(self.needle)\n",
    "        tokens_context = self.encode_text_to_tokens(context)\n",
    "\n",
    "        # Reducing the context length by 150 buffer. This is to account for system message, the user question, and response.\n",
    "        context_length -= self.final_context_length_buffer\n",
    "\n",
    "        # If your context + needle are longer than the context length (which it will be), then reduce tokens from the context by the needle length\n",
    "        if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "            tokens_context = tokens_context[: context_length - len(tokens_needle)]\n",
    "\n",
    "        if depth_percent == 100:\n",
    "            # If your depth percent is 100 (which means your needle is the last thing in the doc), throw it at the end\n",
    "            tokens_new_context = tokens_context + tokens_needle\n",
    "        else:\n",
    "            insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "\n",
    "            tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "            print(f\"Insertion at {insertion_point} / {len(tokens_context)}\")\n",
    "            tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "        # Convert back to a string and return it\n",
    "        new_context = self.decode_tokens(tokens_new_context)\n",
    "        return new_context\n",
    "\n",
    "    def get_context_length_in_tokens(self, context):\n",
    "        return len(self.enc.encode(context))\n",
    "\n",
    "    def read_context_files(self):\n",
    "        context = \"\"\n",
    "        max_context_length = max(self.context_lengths)\n",
    "\n",
    "        while self.get_context_length_in_tokens(context) < max_context_length:\n",
    "            for file in glob.glob(f\"{self.haystack_dir}/*.txt\"):\n",
    "                with open(file, \"r\") as f:\n",
    "                    context += f.read()\n",
    "        return context\n",
    "\n",
    "    def get_tokens_from_context(self, context):\n",
    "        return self.enc.encode(context)\n",
    "\n",
    "    def decode_tokens(self, tokens, context_length=None):\n",
    "        return self.enc.decode(tokens[:context_length], skip_special_tokens=True)\n",
    "\n",
    "    def encode_and_trim(self, context, context_length):\n",
    "        tokens = self.get_tokens_from_context(context)\n",
    "        if len(tokens) > context_length:\n",
    "            context = self.decode_tokens(tokens, context_length)\n",
    "        return context\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.testing_results\n",
    "\n",
    "    def print_start_test_summary(self):\n",
    "        print(\"\\n\")\n",
    "        print(\"Starting Needle In A Haystack Testing...\")\n",
    "        print(f\"- Model: {self.model_name}\")\n",
    "        print(\n",
    "            f\"- Context Lengths: {len(self.context_lengths)}, Min: {min(self.context_lengths)}, Max: {max(self.context_lengths)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"- Document Depths: {len(self.document_depth_percents)}, Min: {min(self.document_depth_percents)}%, Max: {max(self.document_depth_percents)}%\"\n",
    "        )\n",
    "        print(f\"- Needle: {self.needle.strip()}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    def start_test(self, args):\n",
    "        if self.print_ongoing_status:\n",
    "            self.print_start_test_summary()\n",
    "        # asyncio.run(self.run_test())\n",
    "        self.run_test(args)\n",
    "\n",
    "\n",
    "ht = LLMNeedleHaystackTester()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a664fd7-53bb-467f-bac1-95bb5eb991bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
