{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34759c07-f3e3-481b-a2b7-3268b50a09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is adapted from \n",
    "https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031704f1-2b4f-4bb3-b28f-4816671c64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from models/Llama-3-8B-Instruct-Gradient-1048k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd41434758ac428fbf082181371e3bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_lengths_min=120000\n",
    "context_lengths_max=1048000\n",
    "context_lengths_num_intervals=40\n",
    "pretrained_len=1048000\n",
    "sparsity=0.5\n",
    "document_depth_percent_min=0\n",
    "document_depth_percent_max=100\n",
    "document_depth_percent_intervals=10\n",
    "document_depth_percent_interval_type=\"linear\"\n",
    "final_context_length_buffer=200\n",
    "simulation_length=50\n",
    "prefilling_chunk_size=32000\n",
    "\n",
    "needle=\"\\n\\nRemember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\\n\"\n",
    "retrieval_question=\"what is the best thing to do in San Francisco?\\n\\nAnswer: The best thing to do in San Francisco is\"\n",
    "haystack_dir=\"eval/needle/PaulGrahamEssays\"\n",
    "testing_results = []\n",
    "\n",
    "context_lengths = np.round(\n",
    "    np.linspace(\n",
    "        context_lengths_min,\n",
    "        context_lengths_max,\n",
    "        num=context_lengths_num_intervals,\n",
    "        endpoint=True,\n",
    "    )\n",
    ").astype(int)\n",
    "\n",
    "if document_depth_percent_interval_type == \"linear\":\n",
    "    document_depth_percents = np.round(\n",
    "        np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            num=document_depth_percent_intervals,\n",
    "            endpoint=True,\n",
    "        )\n",
    "    ).astype(int)\n",
    "elif document_depth_percent_interval_type == \"sigmoid\":\n",
    "    document_depth_percents = [\n",
    "        logistic(x)\n",
    "        for x in np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            document_depth_percent_intervals,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "model_name = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "model_to_test_description = model_name\n",
    "enc = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "if enc.pad_token_id is None:\n",
    "    if enc.eos_token_id is not None:\n",
    "        enc.pad_token_id = enc.eos_token_id\n",
    "    else:\n",
    "        enc.pad_token_id = 0\n",
    "print(\"Loading from %s\" % model_name)\n",
    "\n",
    "model_to_test = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525bd5fd-fa45-4387-8df7-fd34c3555603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = model_to_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f72801-6cd7-4429-bb79-a933519e10a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting Needle In A Haystack Testing...\n",
      "- Model: models/Llama-3-8B-Instruct-Gradient-1048k\n",
      "- Context Lengths: 40, Min: 120000, Max: 1048000\n",
      "- Document Depths: 10, Min: 0%, Max: 100%\n",
      "- Needle: Remember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Starting Needle In A Haystack Testing...\")\n",
    "print(f\"- Model: {model_name}\")\n",
    "print(\n",
    "    f\"- Context Lengths: {len(context_lengths)}, Min: {min(context_lengths)}, Max: {max(context_lengths)}\"\n",
    ")\n",
    "print(\n",
    "    f\"- Document Depths: {len(document_depth_percents)}, Min: {min(document_depth_percents)}%, Max: {max(document_depth_percents)}%\"\n",
    ")\n",
    "print(f\"- Needle: {needle.strip()}\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c366b54a-5a42-4589-b1ca-392e9da55612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "0\n",
      "-- Test Summary -- \n",
      "Duration: 26.6 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 0%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_0_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_0_results.json\n",
      "11\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 11%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_1100_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_1100_results.json\n",
      "22\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 22%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_2200_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_2200_results.json\n",
      "33\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 33%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_3300_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_3300_results.json\n",
      "44\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 44%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_4400_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_4400_results.json\n",
      "56\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 56%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_5600_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_5600_results.json\n",
      "67\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 67%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_6700_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_6700_results.json\n",
      "78\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 78%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_7800_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_7800_results.json\n",
      "89\n",
      "-- Test Summary -- \n",
      "Duration: 25.9 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 89%\n",
      "Score: 7.0588235294117645\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_8900_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_8900_results.json\n",
      "100\n",
      "-- Test Summary -- \n",
      "Duration: 26.1 seconds\n",
      "Context: 120000 tokens\n",
      "Depth: 100%\n",
      "Score: 6.857142857142856\n",
      "Response: eat a sandwich and sit in Dolores Park on a sunny day.</s>\n",
      "\n",
      "Writing at results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_10000_results.json\n",
      "results/Llama-3-8B-Instruct-Gradient-1048k/Llama-3-8B-Instruct-Gradient-1048k_len_120000_depth_10000_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "\n",
    "def get_context_length_in_tokens(context):\n",
    "    return len(enc.encode(context))\n",
    "    \n",
    "def read_context_files():\n",
    "    context = \"\"\n",
    "    max_context_length = max(context_lengths)\n",
    "    while get_context_length_in_tokens(context) < max_context_length:\n",
    "        for file in glob.glob(f\"{haystack_dir}/*.txt\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                context += f.read()\n",
    "    return context\n",
    "    \n",
    "def get_tokens_from_context(context):\n",
    "    return enc.encode(context)\n",
    "    \n",
    "def decode_tokens(tokens, context_length=None):\n",
    "    return enc.decode(tokens[:context_length], skip_special_tokens=True)\n",
    "    \n",
    "def encode_and_trim(context, context_length):\n",
    "    tokens = get_tokens_from_context(context)\n",
    "    if len(tokens) > context_length:\n",
    "        context = decode_tokens(tokens, context_length)\n",
    "    return context\n",
    "\n",
    "def encode_text_to_tokens(text):\n",
    "    return enc.encode(text, add_special_tokens=False)\n",
    "    \n",
    "def insert_needle(context, depth_percent, context_length):\n",
    "    tokens_needle = encode_text_to_tokens(needle)\n",
    "    tokens_context = encode_text_to_tokens(context)\n",
    "\n",
    "    # Reducing the context length by 150 buffer. This is to account for system message, the user question, and response.\n",
    "    context_length -= final_context_length_buffer\n",
    "\n",
    "    # If your context + needle are longer than the context length (which it will be), then reduce tokens from the context by the needle length\n",
    "    if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "        tokens_context = tokens_context[: context_length - len(tokens_needle)]\n",
    "\n",
    "    if depth_percent == 100:\n",
    "        # If your depth percent is 100 (which means your needle is the last thing in the doc), throw it at the end\n",
    "        tokens_new_context = tokens_context + tokens_needle\n",
    "    else:\n",
    "        insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "\n",
    "        tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "        tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "    # Convert back to a string and return it\n",
    "    new_context = decode_tokens(tokens_new_context)\n",
    "    return new_context\n",
    "    \n",
    "def generate_context(context_length, depth_percent):\n",
    "    # Load up tiktoken so we navigate tokens more easily\n",
    "\n",
    "    # Get your Paul Graham files loaded into a string\n",
    "    context = read_context_files()\n",
    "\n",
    "    # Truncate the Paul Graham essays to the context length you desire\n",
    "    context = encode_and_trim(context, context_length)\n",
    "\n",
    "    # Insert your random statement according to your depth percent\n",
    "    context = insert_needle(context, depth_percent, context_length)\n",
    "\n",
    "    return context\n",
    "    \n",
    "def generate_prompt(context):\n",
    "    test_format = f\"<|im_start|> This is a very long story book: <book> {context} </book>.\\n\\nQuestion: Based on the content of the book, {retrieval_question}\"\n",
    "    return test_format\n",
    "    \n",
    "def bound_evaluate_and_log(context_length, depth_percent):\n",
    "    # Go generate the required length context and place your needle statement in\n",
    "    context = generate_context(context_length, depth_percent)\n",
    "    \n",
    "    # Prepare your message to send to the model you're going to evaluate\n",
    "    prompt = generate_prompt(context)\n",
    "\n",
    "    generated_prompt = prompt\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    # Simulate multiround conversation\n",
    "    prompt = enc(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    prompt_input_ids = prompt[\"input_ids\"].to(model_to_test.device)\n",
    "\n",
    "    simulation_start_idx = prompt_input_ids.size(1) - simulation_length\n",
    "\n",
    "    # question_input_ids = prompt_input_ids[:, simulation_start_idx:]\n",
    "    # prompt_input_ids = prompt_input_ids[:, :simulation_start_idx]\n",
    "    # print(prompt_input_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if prefilling_chunk_size is not None:\n",
    "            past_key_values = DynamicCache()\n",
    "            for i in range(\n",
    "                0, prompt_input_ids.size(1), prefilling_chunk_size\n",
    "            ):\n",
    "                chunk = prompt_input_ids[:, i : i + prefilling_chunk_size]\n",
    "                output = model_to_test(\n",
    "                    input_ids=chunk,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                past_key_values = output.past_key_values\n",
    "        else:\n",
    "            output = model_to_test(\n",
    "                input_ids=prompt_input_ids, past_key_values=None, use_cache=True\n",
    "            )\n",
    "            past_key_values = output.past_key_values\n",
    "\n",
    "        # for input_id in question_input_ids[0]:\n",
    "        #     output = model_to_test(\n",
    "        #         input_ids=input_id.unsqueeze(0).unsqueeze(0),\n",
    "        #         past_key_values=past_key_values,\n",
    "        #         use_cache=True,\n",
    "        #     )\n",
    "        #     past_key_values = output.past_key_values\n",
    "\n",
    "        pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        generated_content = [pred_token_idx.item()]\n",
    "        for _ in range(50):\n",
    "            outputs = model_to_test(\n",
    "                input_ids=pred_token_idx,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "            pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "            generated_content += [pred_token_idx.item()]\n",
    "            if pred_token_idx.item() in eos_token_ids:\n",
    "                break\n",
    "\n",
    "    response = enc.decode(generated_content, skip_special_tokens=True).strip()\n",
    "    \n",
    "    test_end_time = time.time()\n",
    "    test_elapsed_time = test_end_time - test_start_time\n",
    "    score = scorer.score(needle, response)[\"rouge1\"].fmeasure * 10\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "        \"model\": model_to_test_description,\n",
    "        \"context_length\": int(context_length),\n",
    "        \"depth_percent\": float(depth_percent),\n",
    "        \"needle\": needle,\n",
    "        \"model_response\": response,\n",
    "        \"score\": score,\n",
    "        \"test_duration_seconds\": test_elapsed_time,\n",
    "        \"test_timestamp_utc\": datetime.now(timezone.utc).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S%z\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    testing_results.append(results)\n",
    "    print(f\"-- Test Summary -- \")\n",
    "    print(f\"Duration: {test_elapsed_time:.1f} seconds\")\n",
    "    print(f\"Context: {context_length} tokens\")\n",
    "    print(f\"Depth: {depth_percent}%\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "    model_version = model_name.split(\"/\")[-1]\n",
    "    context_file_location = f'{model_version.replace(\".\", \"_\")}_len_{context_length}_depth_{int(depth_percent*100)}'\n",
    "\n",
    "    results[\"file_name\"] = context_file_location\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"contexts\"):\n",
    "        os.makedirs(\"contexts\")\n",
    "\n",
    "    if not os.path.exists(f\"contexts/{model_version}\"):\n",
    "        os.makedirs(f\"contexts/{model_version}\")\n",
    "\n",
    "    with open(\n",
    "        f\"contexts/{model_version}/{context_file_location}_context.txt\",\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        f.write(context)\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    if not os.path.exists(f\"results/{model_version}\"):\n",
    "        os.makedirs(f\"results/{model_version}\")\n",
    "\n",
    "    # Save the result to file for retesting\n",
    "    p = f\"results/{model_version}/{context_file_location}_results.json\"\n",
    "    print(\"Writing at %s\" % p)\n",
    "    print(p)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return None, generated_prompt\n",
    "\n",
    "s_len = 1\n",
    "e_len = pretrained_len\n",
    "tasks = []\n",
    "for context_length in context_lengths:\n",
    "    print(context_length)\n",
    "    if context_length < s_len or context_length > e_len:\n",
    "        continue\n",
    "    for depth_percent in document_depth_percents:\n",
    "        print(depth_percent)\n",
    "        task = bound_evaluate_and_log(context_length, depth_percent)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a2f8-9020-4828-9f4f-84adafac2cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60aa097-3346-48e4-aa0c-b5a3262da9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9106f27-dbe0-4745-8e23-c95d5d19cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
