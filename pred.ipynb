{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17be85e-eb43-44cc-a804-333b2a78b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4375ca26-e1f3-471f-83e5-91e6d5992b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe6cc14-08b7-4622-87a7-91e4a5584676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Llama-2-7B-32K-Instruct': 'models/Llama-2-7B-32K-Instruct', 'Mistral-7B-Instruct-v0.2': 'models/Mistral-7B-Instruct-v0.2', 'Mistral-7B-Instruct-v0.3': 'models/Mistral-7B-Instruct-v0.3', 'Llama-3-8B-Instruct-Gradient-1048k': 'models/Llama-3-8B-Instruct-Gradient-1048k', 'Meta-Llama-3.1-8B-Instruct': 'models/Meta-Llama-3.1-8B-Instruct'}\n",
      "{'Mistral-7B-Instruct-v0.2': 31500, 'Mistral-7B-Instruct-v0.3': 31500, 'Llama-3-8B-Instruct-Gradient-1048k': 1047500, 'Llama-2-7B-32K-Instruct': 31500, 'Meta-Llama-3.1-8B-Instruct': 127500}\n"
     ]
    }
   ],
   "source": [
    "model2path = json.load(open(\"eval/LongBench/config/model2path.json\", \"r\"))\n",
    "model2maxlen = json.load(open(\"eval/LongBench/config/model2maxlen.json\", \"r\"))\n",
    "print(model2path)\n",
    "print(model2maxlen)\n",
    "\n",
    "model_name = 'Llama-3-8B-Instruct-Gradient-1048k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a128bc16-f2c1-49b5-9a78-a1fdd0597ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c94a9c2e9d4c59adfd910f6aed362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_model_and_tokenizer(path, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        path, trust_remote_code=True, use_fast=False\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(path)\n",
    "    eos_token_ids = generation_config.eos_token_id\n",
    "    if not isinstance(eos_token_ids, list):\n",
    "        eos_token_ids = [eos_token_ids]\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    return model, tokenizer, eos_token_ids\n",
    "\n",
    "model, tokenizer, eos_token_ids = load_model_and_tokenizer(\n",
    "    model2path[model_name], model_name\n",
    ")\n",
    "\n",
    "from duo_attn.utils import to_device\n",
    "device_list = [i for i in range(torch.cuda.device_count())]\n",
    "model = to_device(model, device_list, enable_tp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c10079-54fb-45f3-bbeb-8862b7d145b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model2maxlen[model_name]\n",
    "datasets = [\n",
    "            \"qasper\",\n",
    "            \"multifieldqa_en\",\n",
    "            \"hotpotqa\",\n",
    "            \"2wikimqa\",\n",
    "            \"gov_report\",\n",
    "            \"multi_news\",\n",
    "            \"trec\",\n",
    "            \"triviaqa\",\n",
    "            \"samsum\",\n",
    "            \"passage_count\",\n",
    "            \"passage_retrieval_en\",\n",
    "            \"lcc\",\n",
    "            \"repobench-p\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbd9ede-3ad2-46f2-bc37-043a6ea3d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset2prompt = json.load(open(\"eval/LongBench/config/dataset2prompt.json\", \"r\"))\n",
    "dataset2maxlen = json.load(open(\"eval/LongBench/config/dataset2maxlen.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f01e0a-6291-4249-ad9e-345722b8b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"LongBench/pred\"):\n",
    "    os.makedirs(\"LongBench/pred\")\n",
    "if not os.path.exists(\"LongBench/pred_e\"):\n",
    "    os.makedirs(\"LongBench/pred_e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4db66fc-21dd-48f5-8557-48d7d316ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the customized building prompt for chat models\n",
    "def build_chat(tokenizer, prompt, model_name):\n",
    "    if \"llama-2\" in model_name:\n",
    "        prompt = f\"[INST]{prompt}[/INST]\"\n",
    "    return prompt\n",
    "\n",
    "def post_process(response, model_name):\n",
    "    if \"xgen\" in model_name:\n",
    "        response = response.strip().replace(\"Assistant:\", \"\")\n",
    "    elif \"internlm\" in model_name:\n",
    "        response = response.split(\"<eoa>\")[0]\n",
    "    elif \"llama-3\" in model_name.lower():\n",
    "        response = (\n",
    "            response.split(\".assistant\")[0]\n",
    "            .split(\"\\n\\nQuestion\")[0]\n",
    "            .split(\"</s>\")[0]\n",
    "            .strip()\n",
    "        )\n",
    "    elif \"Llama-2-7B-32K-Instruct\" in model_name:\n",
    "        response = (\n",
    "            response.split(\"(Document\")[0]\n",
    "            .split(\"\\n\\nQuestion\")[0]\n",
    "            .split(\"\\n\\nAnswer\")[0]\n",
    "            .split(\"(Passage\")[0]\n",
    "            .strip()\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def get_pred(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eos_token_ids,\n",
    "    data,\n",
    "    max_length,\n",
    "    max_gen,\n",
    "    prompt_format,\n",
    "    dataset,\n",
    "    model_name,\n",
    "    decoding_simulation_length,\n",
    "):\n",
    "    preds = []\n",
    "    pbar = tqdm(data)\n",
    "    for idx, json_obj in enumerate(pbar):\n",
    "        prompt = prompt_format.format(**json_obj)\n",
    "        # truncate to fit max_length (we suggest truncate in the middle, since the left and right side may contain crucial instructions)\n",
    "        tokenized_prompt = tokenizer(\n",
    "            prompt, truncation=False, return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        if len(tokenized_prompt) > max_length:\n",
    "            half = int(max_length / 2)\n",
    "            prompt = tokenizer.decode(\n",
    "                tokenized_prompt[:half], skip_special_tokens=True\n",
    "            ) + tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)\n",
    "        if dataset not in [\n",
    "            \"trec\",\n",
    "            \"triviaqa\",\n",
    "            \"samsum\",\n",
    "            \"lsht\",\n",
    "            \"lcc\",\n",
    "            \"repobench-p\",\n",
    "        ]:  # chat models are better off without build prompts on these tasks\n",
    "            prompt = build_chat(tokenizer, prompt, model_name)\n",
    "\n",
    "        input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "        pbar.set_description(\n",
    "            f\"Generating for {idx}, len = {input.input_ids.shape[-1]}\"\n",
    "        )\n",
    "        simulation_start_idx = input.input_ids.shape[-1] - decoding_simulation_length\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=input.input_ids[:, :simulation_start_idx],\n",
    "                past_key_values=None,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            past_key_values = output.past_key_values\n",
    "            if decoding_simulation_length > 0:\n",
    "                for idx, input_id in enumerate(\n",
    "                    input.input_ids[0, simulation_start_idx:]\n",
    "                ):\n",
    "                    output = model(\n",
    "                        input_ids=input_id.unsqueeze(0).unsqueeze(0),\n",
    "                        past_key_values=past_key_values,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                    past_key_values = output.past_key_values\n",
    "            pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "            generated_content = [pred_token_idx.item()]\n",
    "            for _ in range(max_gen - 1):\n",
    "                outputs = model(\n",
    "                    input_ids=pred_token_idx,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "\n",
    "                past_key_values = outputs.past_key_values\n",
    "                pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "                generated_content += [pred_token_idx.item()]\n",
    "                if pred_token_idx.item() in eos_token_ids:\n",
    "                    break\n",
    "\n",
    "        pred = tokenizer.decode(generated_content, skip_special_tokens=True)\n",
    "        pred = post_process(pred, model_name)\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        preds.append(\n",
    "            {\n",
    "                \"pred\": pred,\n",
    "                \"answers\": json_obj[\"answers\"],\n",
    "                \"all_classes\": json_obj[\"all_classes\"],\n",
    "                \"length\": json_obj[\"length\"],\n",
    "            }\n",
    "        )\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace27760-fa14-4198-8fe8-3674571bd692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 0, len = 4000:   0%|          | 0/200 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "Generating for 1, len = 3331:   0%|          | 1/200 [00:03<12:51,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The ground truth for fake news is established through manual inspection of the text field within the tweets to label them as containing fake news, or not containing them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 2, len = 4361:   1%|          | 2/200 [00:08<14:15,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The GhostVLAD approach is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and excludes them during feature aggregation. It is used for language identification and has been shown to outperform other pooling methods by achieving 98.43% F1-Score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 3, len = 2977:   2%|▏         | 3/200 [00:15<18:32,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Their model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 4, len = 4439:   2%|▏         | 4/200 [00:19<16:12,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The article proposes using context tweets as an additional feature for neural network models to better understand the data and improve the accuracy of detecting abusive language. The article also suggests using ensemble models of variant models and features for further improvements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 5, len = 5373:   2%|▎         | 5/200 [00:25<16:59,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: They looked at different Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. They also used a subset of pages based on their performance on the development set and the observation of emotions distribution on different pages and in the different datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 6, len = 5635:   3%|▎         | 6/200 [00:30<17:04,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Yes. The article states that the hashtag segmentation model is language-independent and the authors intend to extend their toolkit to languages other than English as future work. However, the article also mentions that the authors focused on English hashtags in their experiments. Therefore, the answer to this question is \"yes\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating for 7, len = 6498:   4%|▎         | 7/200 [00:38<19:38,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: The article proposes an evaluation protocol and baseline for the task of concept-map-based MDS. The corpus is also publicly available under a permissive license.\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    data = load_dataset(\"THUDM/LongBench\", dataset, split=\"test\")\n",
    "    if not os.path.exists(f\"LongBench/pred/{model_name}\"):\n",
    "        os.makedirs(f\"LongBench/pred/{model_name}\")\n",
    "    out_path = f\"LongBench/pred/{model_name}/{dataset}-full.jsonl\"\n",
    "    prompt_format = dataset2prompt[dataset]\n",
    "    max_gen = dataset2maxlen[dataset]\n",
    "\n",
    "    preds = get_pred(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            eos_token_ids,\n",
    "            data,\n",
    "            max_length,\n",
    "            max_gen,\n",
    "            prompt_format,\n",
    "            dataset,\n",
    "            model_name,\n",
    "            50,\n",
    "        )\n",
    "    print(preds)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for pred in preds:\n",
    "            json.dump(pred, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30736f-e1fb-4573-aec8-4f7879b24262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
