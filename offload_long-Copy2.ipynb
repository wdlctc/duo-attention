{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ee6126-f815-4456-85ca-459177f91cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbdb02b3db3459b9593f545a1322b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# Load the model\n",
    "ckpt = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(ckpt)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "# add some tokens like \"</user>\" and </s> to eos ids\n",
    "eos_token_ids += tokenizer.encode(\"</user>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</s>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209737de-4786-467f-b0d1-3c75c2a6cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    logger,\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    "    LlamaSdpaAttention,\n",
    "    LlamaFlashAttention2,\n",
    "    LlamaMLP\n",
    ")\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_flash_attention_utils  import _flash_attention_forward\n",
    "\n",
    "def minis_mlp_forward(self, x):\n",
    "    bsz, q_len, _ = x.size()\n",
    "    chunk_size = self.hidden_size\n",
    "\n",
    "    x_list = list(x.split(chunk_size, dim=1))\n",
    "\n",
    "    output_list = [None for _ in range(len(x_list))]\n",
    "\n",
    "    for i in range(len(x_list)):\n",
    "        x = x_list[i]\n",
    "        x_list[i] = None\n",
    "        output_list[i] = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "    down_proj = torch.cat(output_list, dim=1)\n",
    "\n",
    "    return down_proj\n",
    "    \n",
    "LlamaMLP.forward = minis_mlp_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d56277-e2e4-4873-9c0f-2fbb3a66f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def LlamaAttention_fast_forward(\n",
    "#     self,\n",
    "#     hidden_states: torch.Tensor,\n",
    "#     attention_mask: Optional[torch.Tensor] = None,\n",
    "#     position_ids: Optional[torch.LongTensor] = None,\n",
    "#     past_key_value: Optional[Cache] = None,\n",
    "#     output_attentions: bool = False,\n",
    "#     use_cache: bool = False,\n",
    "#     cache_position: Optional[torch.LongTensor] = None,\n",
    "#     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "#     **kwargs,\n",
    "# ) :\n",
    "#     output_attentions = False\n",
    "\n",
    "#     bsz, q_len, hd = hidden_states.size()\n",
    "#     chunk_size = hd // self.num_key_value_heads\n",
    "#     num_heads = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "#     if not hasattr(self, 'q_proj_list'):\n",
    "#         self.q_proj_list = list((self.q_proj.weight.split(self.head_dim * num_heads, dim=0)))\n",
    "#         # self.q_proj.weight.data.storage().resize_(0)\n",
    "#     if not hasattr(self, 'k_proj_list'):\n",
    "#         self.k_proj_list = list((self.k_proj.weight.split(self.head_dim, dim=0)))\n",
    "#         # self.k_proj.weight.data.storage().resize_(0)\n",
    "#     if not hasattr(self, 'v_proj_list'):\n",
    "#         self.v_proj_list = list((self.v_proj.weight.split(self.head_dim, dim=0)))\n",
    "#         # self.v_proj.weight.data.storage().resize_(0)\n",
    "\n",
    "\n",
    "#     attn_output_list = [None for _ in range((self.num_key_value_heads))]\n",
    "    \n",
    "#     for i in range(self.num_key_value_heads):\n",
    "#         bsz, q_len, hd = hidden_states.size()\n",
    "\n",
    "#         self.q_proj.weight.data = self.q_proj_list[i].data\n",
    "#         self.k_proj.weight.data = self.k_proj_list[i].data\n",
    "#         self.v_proj.weight.data = self.v_proj_list[i].data\n",
    "\n",
    "#         # print(hidden_states.shape, self.q_proj.weight.shape)\n",
    "        \n",
    "#         query_states = self.q_proj(hidden_states)\n",
    "#         key_states = self.k_proj(hidden_states)\n",
    "#         value_states = self.v_proj(hidden_states)\n",
    "\n",
    "#         # Flash attention requires the input to have the shape\n",
    "#         # batch_size x seq_length x head_dim x hidden_dim\n",
    "#         # therefore we just need to keep the original shape\n",
    "#         query_states = query_states.view(bsz, q_len, num_heads, self.head_dim).transpose(1, 2)\n",
    "#         key_states = key_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "#         value_states = value_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "#         if position_embeddings is None:\n",
    "#             logger.warning_once(\n",
    "#                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "#                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "#                 \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "#                 \"removed and `position_embeddings` will be mandatory.\"\n",
    "#             )\n",
    "#             cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "#         else:\n",
    "#             cos, sin = position_embeddings\n",
    "#         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "    \n",
    "#         if past_key_value is not None:\n",
    "#             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "#             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "#             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx + i, cache_kwargs)\n",
    "    \n",
    "#         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
    "#         # to be able to avoid many of these transpose/reshape/view.\n",
    "#         query_states = query_states.transpose(1, 2)\n",
    "#         key_states = key_states.transpose(1, 2)\n",
    "#         value_states = value_states.transpose(1, 2)\n",
    "    \n",
    "#         dropout_rate = self.attention_dropout if self.training else 0.0\n",
    "    \n",
    "#         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
    "#         # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
    "#         # cast them back in the correct dtype just to be sure everything works as expected.\n",
    "#         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
    "#         # in fp32. (LlamaRMSNorm handles it correctly)\n",
    "    \n",
    "#         input_dtype = query_states.dtype\n",
    "#         if input_dtype == torch.float32:\n",
    "#             if torch.is_autocast_enabled():\n",
    "#                 target_dtype = torch.get_autocast_gpu_dtype()\n",
    "#             # Handle the case where the model is quantized\n",
    "#             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n",
    "#                 target_dtype = self.config._pre_quantization_dtype\n",
    "#             else:\n",
    "#                 target_dtype = self.q_proj.weight.dtype\n",
    "    \n",
    "#             logger.warning_once(\n",
    "#                 f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n",
    "#                 f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n",
    "#                 f\" {target_dtype}.\"\n",
    "#             )\n",
    "    \n",
    "#             query_states = query_states.to(target_dtype)\n",
    "#             key_states = key_states.to(target_dtype)\n",
    "#             value_states = value_states.to(target_dtype)\n",
    "    \n",
    "#         attn_output = _flash_attention_forward(\n",
    "#             query_states,\n",
    "#             key_states,\n",
    "#             value_states,\n",
    "#             attention_mask,\n",
    "#             q_len,\n",
    "#             position_ids=position_ids,\n",
    "#             dropout=dropout_rate,\n",
    "#             sliding_window=getattr(self, \"sliding_window\", None),\n",
    "#             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n",
    "#             is_causal=self.is_causal,\n",
    "#         )\n",
    "    \n",
    "#         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n",
    "#         attn_output_list[i] = attn_output\n",
    "        \n",
    "#     attn_output = torch.cat(attn_output_list, dim=-1)\n",
    "#     attn_output = self.o_proj(attn_output)\n",
    "\n",
    "#     if not output_attentions:\n",
    "#         attn_weights = None\n",
    "\n",
    "#     return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# LlamaFlashAttention2.forward = LlamaAttention_fast_forward\n",
    "# layer_idx = 0\n",
    "# for name, module in model.named_modules():\n",
    "#     if \"self_attn\" in name and hasattr(module, \"q_proj\"):\n",
    "#         module.layer_idx = layer_idx\n",
    "#         layer_idx += module.num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cd715d-1e47-4869-bb3f-4a10e1c7467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "class OffloadedCache(DynamicCache):\n",
    "    \"\"\"\n",
    "    A drop-in replacement for DynamicCache that conserves GPU memory at the expense of more CPU memory.\n",
    "    Useful for generating from models with very long context.\n",
    "\n",
    "    In addition to the default CUDA stream, where all forward() computations happen,\n",
    "    this class uses another stream, the prefetch stream, which it creates itself.\n",
    "    Since scheduling of operations on separate streams happens independently, this class uses\n",
    "    the prefetch stream to asynchronously prefetch the KV cache of layer k+1 when layer k is executing.\n",
    "    The movement of the layer k-1 cache to the CPU is handled by the default stream as a simple way to\n",
    "    ensure the eviction is scheduled after all computations on that cache are finished.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"OffloadedCache can only be used with a GPU\")\n",
    "        super().__init__()\n",
    "        self.original_device = []\n",
    "        self.prefetch_stream = torch.cuda.Stream()\n",
    "        self.beam_idx = None  # used to delay beam search operations\n",
    "\n",
    "        self.proj = nn.Linear(128, 128, bias=False, dtype=torch.bfloat16).cuda()\n",
    "\n",
    "    def prefetch_layer(self, layer_idx: int):\n",
    "        \"Starts prefetching the next layer cache\"\n",
    "        if layer_idx < len(self):\n",
    "            with torch.cuda.stream(self.prefetch_stream):\n",
    "                # Prefetch next layer tensors to GPU\n",
    "                device = self.original_device[layer_idx]\n",
    "                self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device, non_blocking=True)\n",
    "                self.value_cache[layer_idx] = self.value_cache[layer_idx].to(device, non_blocking=True)\n",
    "\n",
    "    def evict_previous_layer(self, layer_idx: int):\n",
    "        \"Moves the previous layer cache to the CPU\"\n",
    "        if len(self) > 2:\n",
    "            # We do it on the default stream so it occurs after all earlier computations on these tensors are done\n",
    "            prev_layer_idx = (layer_idx - 1) % len(self)\n",
    "            self.key_cache[prev_layer_idx] = self.key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            self.value_cache[prev_layer_idx] = self.value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n",
    "        if layer_idx < len(self):\n",
    "            # Evict the previous layer if necessary\n",
    "            torch.cuda.current_stream().synchronize()\n",
    "            self.evict_previous_layer(layer_idx)\n",
    "            # Load current layer cache to its original device if not already there\n",
    "            original_device = self.original_device[layer_idx]\n",
    "            self.prefetch_stream.synchronize()\n",
    "            key_tensor = self.key_cache[layer_idx]\n",
    "            value_tensor = self.value_cache[layer_idx]\n",
    "            # Now deal with beam search ops which were delayed\n",
    "            if self.beam_idx is not None:\n",
    "                self.beam_idx = self.beam_idx.to(original_device)\n",
    "                key_tensor = key_tensor.index_select(0, self.beam_idx)\n",
    "                value_tensor = value_tensor.index_select(0, self.beam_idx)\n",
    "            # Prefetch the next layer\n",
    "            self.prefetch_layer((layer_idx + 1) % len(self))\n",
    "            return (key_tensor, value_tensor)\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        \"\"\"Saves the beam indices and reorders the cache when the tensor is back to its device.\"\"\"\n",
    "        # We delay this operation until the tensors are back to their original\n",
    "        # device because performing torch.index_select on the CPU is very slow\n",
    "        del self.beam_idx\n",
    "        self.beam_idx = beam_idx.clone()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `OffloadedCache`.\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) < layer_idx // 8:\n",
    "            raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n",
    "        elif len(self.key_cache) == layer_idx // 8:\n",
    "            if layer_idx % 8 == 0:\n",
    "                layer_idx = layer_idx // 8\n",
    "                self.key_cache.append(key_states)\n",
    "                self.value_cache.append(value_states)\n",
    "                self.original_device.append(key_states.device)\n",
    "                self.evict_previous_layer(layer_idx)\n",
    "            else:\n",
    "                layer_idx = layer_idx // 8 \n",
    "        else:\n",
    "            if layer_idx % 8 == 0:\n",
    "                layer_idx = layer_idx // 8\n",
    "                key_tensor, value_tensor = self[layer_idx]\n",
    "                self.key_cache[layer_idx] = torch.cat([key_tensor, key_states], dim=-2)\n",
    "                self.value_cache[layer_idx] = torch.cat([value_tensor, value_states], dim=-2)\n",
    "            else:\n",
    "                layer_idx = layer_idx // 8 \n",
    "                key = self.proj(self.key_cache[layer_idx])\n",
    "                value = self.proj(self.value_cache[layer_idx])\n",
    "                return key, value\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    # According to https://docs.python.org/3/library/exceptions.html#NotImplementedError\n",
    "    # if a method is not supposed to be supported in a subclass we should set it to None\n",
    "    from_legacy_cache = None\n",
    "\n",
    "    to_legacy_cache = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6582c1-ade6-452b-b6f6-2ccd5d29605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Time: 3.90 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 1/10 - Time: 0.16 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 2/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 3/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 4/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 5/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 6/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 7/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 8/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 9/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n",
      "Epoch 10/10 - Time: 0.13 seconds\n",
      "Peak allocated bytes on 20.598151GB\n"
     ]
    }
   ],
   "source": [
    "# Manually perform inference using KV cache\n",
    "input_ids = torch.randint(0, tokenizer.vocab_size, (1, 32000)).to('cuda')\n",
    "next_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 1)).to('cuda')\n",
    "\n",
    "max_new_tokens = 1\n",
    "next_new_tokens = 10\n",
    "generated_tokens = []\n",
    "\n",
    "# # Initialize past_key_values to None\n",
    "# past_key_values = OffloadedCache()\n",
    "# past_key_values.sink_size = 64\n",
    "# past_key_values.recent_size = 256\n",
    "\n",
    "# config = model.config\n",
    "# for idx, layer in enumerate(model.model.layers):\n",
    "#     device = next(model.parameters()).device\n",
    "#     dtype = next(model.parameters()).dtype\n",
    "#     module = layer.self_attn\n",
    "\n",
    "#     # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n",
    "#     # head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n",
    "#     head_dim = config.hidden_size\n",
    "    \n",
    "#     cache_shape = (1, 1, 32000 * (max_new_tokens)  + 15, head_dim)\n",
    "#     key_states = torch.zeros(cache_shape, dtype=dtype, device=device)\n",
    "#     value_states = torch.zeros(cache_shape, dtype=dtype, device=device)\n",
    "\n",
    "#     # for i in range(config.num_key_value_heads):\n",
    "#     cache_kwargs = {'full_head': None}\n",
    "#     key_states, value_states = past_key_values.update(key_states, value_states, module.layer_idx, cache_kwargs)\n",
    "\n",
    "\n",
    "# Initialize past_key_values to None\n",
    "past_key_values = DynamicCache()\n",
    "\n",
    "for epoch in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{max_new_tokens} - Time: {epoch_time:.2f} seconds\")\n",
    "        print(\n",
    "            \"Peak allocated bytes on {:4f}GB\".format(\n",
    "                torch.cuda.memory_stats(0)[\"allocated_bytes.all.peak\"] / 2**30\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "for epoch in range(next_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids=next_input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{next_new_tokens} - Time: {epoch_time:.2f} seconds\")\n",
    "        print(\n",
    "            \"Peak allocated bytes on {:4f}GB\".format(\n",
    "                torch.cuda.memory_stats(0)[\"allocated_bytes.all.peak\"] / 2**30\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefbfcf-e402-4e8d-8c88-ba476306fb31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
