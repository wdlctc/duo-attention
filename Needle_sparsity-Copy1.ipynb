{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34759c07-f3e3-481b-a2b7-3268b50a09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is adapted from \n",
    "https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031704f1-2b4f-4bb3-b28f-4816671c64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from models/Llama-3-8B-Instruct-Gradient-1048k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be35065495bb429798234fe651e804be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_lengths_min=120000\n",
    "context_lengths_max=1048000\n",
    "context_lengths_num_intervals=40\n",
    "pretrained_len=1048000\n",
    "sparsity=0.5\n",
    "document_depth_percent_min=0\n",
    "document_depth_percent_max=100\n",
    "document_depth_percent_intervals=10\n",
    "document_depth_percent_interval_type=\"linear\"\n",
    "final_context_length_buffer=200\n",
    "simulation_length=50\n",
    "prefilling_chunk_size=32000\n",
    "\n",
    "needle=\"\\n\\nRemember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\\n\"\n",
    "retrieval_question=\"what is the best thing to do in San Francisco?\\n\\nAnswer: The best thing to do in San Francisco is\"\n",
    "haystack_dir=\"eval/needle/PaulGrahamEssays\"\n",
    "testing_results = []\n",
    "\n",
    "context_lengths = np.round(\n",
    "    np.linspace(\n",
    "        context_lengths_min,\n",
    "        context_lengths_max,\n",
    "        num=context_lengths_num_intervals,\n",
    "        endpoint=True,\n",
    "    )\n",
    ").astype(int)\n",
    "\n",
    "if document_depth_percent_interval_type == \"linear\":\n",
    "    document_depth_percents = np.round(\n",
    "        np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            num=document_depth_percent_intervals,\n",
    "            endpoint=True,\n",
    "        )\n",
    "    ).astype(int)\n",
    "elif document_depth_percent_interval_type == \"sigmoid\":\n",
    "    document_depth_percents = [\n",
    "        logistic(x)\n",
    "        for x in np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            document_depth_percent_intervals,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "model_name = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "model_to_test_description = model_name\n",
    "enc = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "if enc.pad_token_id is None:\n",
    "    if enc.eos_token_id is not None:\n",
    "        enc.pad_token_id = enc.eos_token_id\n",
    "    else:\n",
    "        enc.pad_token_id = 0\n",
    "print(\"Loading from %s\" % model_name)\n",
    "\n",
    "model_to_test = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eff818-840f-41de-b18c-54a256b11f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8)\n",
      "128\n",
      "256\n",
      "[[8.59375494e-01 6.52343938e-01 1.00000032e+00 3.39843808e-01\n",
      "  4.09967454e-07 6.79687889e-01 3.49610313e-01 2.73438213e-01]\n",
      " [5.73501730e-07 5.51068665e-07 5.32082884e-07 1.00000024e+00\n",
      "  8.73311763e-07 8.24219616e-01 4.82941833e-07 3.61803403e-07]\n",
      " [7.22682208e-07 6.91406515e-01 4.76164900e-08 9.60938032e-01\n",
      "  7.30469461e-01 9.84375899e-01 9.64844422e-01 9.38280445e-07]\n",
      " [7.85156515e-01 2.34008409e-05 3.90153507e-05 2.30391647e-07\n",
      "  8.71094682e-01 9.76562762e-01 1.00000023e+00 4.21875728e-01]\n",
      " [3.53405988e-07 2.60187280e-07 1.17676117e-01 8.63281454e-01\n",
      "  1.00000087e+00 9.88281270e-01 8.35938315e-01 2.61545667e-04]\n",
      " [7.89063004e-01 9.84375303e-01 1.00000001e+00 5.82031273e-01\n",
      "  1.00000041e+00 9.45312536e-01 9.52149964e-02 1.00000049e+00]\n",
      " [5.00000188e-01 8.63281538e-01 7.03125830e-01 7.18750321e-01\n",
      "  9.10156503e-01 1.00000075e+00 1.00000024e+00 8.94531651e-01]\n",
      " [1.00000022e+00 8.32031354e-01 1.44052526e-02 1.00000059e+00\n",
      "  6.20250915e-05 1.00000031e+00 1.00000069e+00 3.41797548e-01]\n",
      " [1.00000053e+00 1.00000080e+00 1.00000070e+00 1.54297567e-01\n",
      "  1.00000029e+00 4.88281831e-01 9.72656552e-01 1.00000051e+00]\n",
      " [9.14063339e-01 7.65625569e-01 4.88281253e-01 1.00000004e+00\n",
      "  8.39844722e-01 1.00000079e+00 1.00000003e+00 1.00000087e+00]\n",
      " [1.00000067e+00 8.39844570e-01 8.78906692e-01 9.84375781e-01\n",
      "  1.00000008e+00 5.66406383e-01 1.00000043e+00 1.00000085e+00]\n",
      " [1.00000100e+00 1.00000006e+00 8.08594057e-01 9.57031434e-01\n",
      "  9.45313264e-01 8.63281762e-01 7.22662243e-02 1.00000092e+00]\n",
      " [1.00000011e+00 4.83407974e-02 7.26563170e-01 8.71094324e-01\n",
      "  9.37500870e-01 1.00000041e+00 3.97255603e-04 4.49219023e-01]\n",
      " [1.00000015e+00 1.00000042e+00 1.00000047e+00 9.33594525e-01\n",
      "  1.00000028e+00 8.00781359e-01 1.00000004e+00 1.00000002e+00]\n",
      " [6.17187558e-01 1.00000068e+00 5.07812714e-01 9.49218955e-01\n",
      "  9.80469069e-01 9.64844051e-01 9.72656341e-01 1.00000036e+00]\n",
      " [1.00000085e+00 6.52343835e-01 1.00000080e+00 2.34375048e-01\n",
      "  1.00000048e+00 8.47656960e-01 1.00000088e+00 1.00000090e+00]\n",
      " [9.92188065e-01 1.00000075e+00 1.00000056e+00 5.19029839e-07\n",
      "  9.33594380e-01 1.00000032e+00 1.00000037e+00 7.26563327e-01]\n",
      " [1.00000092e+00 1.00000050e+00 9.33593889e-01 1.00000065e+00\n",
      "  7.73437503e-01 9.64844223e-01 1.00000079e+00 9.80469527e-01]\n",
      " [5.19531694e-01 9.96094677e-01 7.96875199e-01 1.00000033e+00\n",
      "  9.17969049e-01 9.64844024e-01 1.39293588e-04 8.51562788e-01]\n",
      " [1.00000016e+00 7.61719366e-01 1.00000054e+00 1.00000010e+00\n",
      "  1.48071634e-03 9.80468829e-01 5.85938137e-01 1.00000011e+00]\n",
      " [9.88281500e-01 5.38225703e-07 1.00000021e+00 9.88281556e-01\n",
      "  1.00000081e+00 9.92187568e-01 1.00000082e+00 2.71484659e-01]\n",
      " [1.00000019e+00 9.10156806e-01 9.72656839e-01 1.00000065e+00\n",
      "  9.25782124e-01 9.57031500e-01 8.08594224e-01 5.78125605e-01]\n",
      " [9.29688064e-01 9.53125291e-01 8.39844604e-01 9.88281929e-01\n",
      "  1.00000047e+00 7.10938216e-01 7.89063212e-01 9.64844132e-01]\n",
      " [1.00000056e+00 1.00000091e+00 3.13157117e-04 9.72657185e-01\n",
      "  3.90692045e-03 9.84375393e-01 9.88281417e-01 1.00000030e+00]\n",
      " [1.00000017e+00 5.11719411e-01 7.15496876e-07 9.37500438e-01\n",
      "  9.76562662e-01 9.88281841e-01 9.92187823e-01 7.34375780e-01]\n",
      " [9.68750834e-01 9.92187606e-01 7.72452369e-05 9.96094332e-01\n",
      "  9.60938116e-01 1.00000075e+00 5.36286625e-08 6.48265707e-05]\n",
      " [1.00000001e+00 9.10156682e-01 6.83594338e-01 1.00000021e+00\n",
      "  9.21875400e-01 8.47656376e-01 8.90625243e-01 9.96093945e-01]\n",
      " [5.95130920e-05 1.00000064e+00 8.90625411e-01 9.53125862e-01\n",
      "  9.45313469e-01 9.96093870e-01 1.00000066e+00 9.88281656e-01]\n",
      " [9.96093751e-01 9.72657024e-01 9.17969599e-01 9.84375756e-01\n",
      "  1.00000023e+00 9.88282123e-01 1.00000087e+00 9.49219079e-01]\n",
      " [3.87197975e-07 1.11705376e-04 1.00000012e+00 1.37568451e-04\n",
      "  1.00000023e+00 1.00000015e+00 1.00000059e+00 1.00000000e+00]\n",
      " [9.92188131e-01 1.00000051e+00 9.96094161e-01 9.96094447e-01\n",
      "  1.00000046e+00 1.00000071e+00 9.96094649e-01 9.96094505e-01]\n",
      " [9.92187897e-01 8.90625292e-01 9.41406921e-01 7.92968954e-01\n",
      "  9.29687823e-01 9.64844023e-01 9.29688107e-01 9.60937909e-01]] [[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]] 0.5\n",
      "Enabling DuoAttention evaluation using sink size 64 and recent size 256\n",
      "Enabling tuple KV cache for Llama\n"
     ]
    }
   ],
   "source": [
    "from duo_attn.utils import load_attn_pattern, sparsify_attention_heads\n",
    "from duo_attn.patch import enable_duo_attention_eval\n",
    "\n",
    "# Load the attention pattern\n",
    "attn_heads, sink_size, recent_size = load_attn_pattern(\n",
    "    \"attn_patterns/Llama-3-8B-Instruct-Gradient-1048k/lr=0.02-reg=0.05-ctx=1000_32000-multi_passkey10\"\n",
    ")\n",
    "\n",
    "print(attn_heads.shape)\n",
    "print(sink_size)\n",
    "print(recent_size)\n",
    "\n",
    "# Sparsify attention heads\n",
    "attn_heads_after, sparsity = sparsify_attention_heads(attn_heads, sparsity=0.5)\n",
    "\n",
    "print(attn_heads, attn_heads_after, sparsity)\n",
    "\n",
    "enable_duo_attention_eval(\n",
    "    model_to_test,\n",
    "    attn_heads_after,\n",
    "    sink_size=64,\n",
    "    recent_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525bd5fd-fa45-4387-8df7-fd34c3555603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = model_to_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f72801-6cd7-4429-bb79-a933519e10a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting Needle In A Haystack Testing...\n",
      "- Model: models/Llama-3-8B-Instruct-Gradient-1048k\n",
      "- Context Lengths: 40, Min: 120000, Max: 1048000\n",
      "- Document Depths: 10, Min: 0%, Max: 100%\n",
      "- Needle: Remember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Starting Needle In A Haystack Testing...\")\n",
    "print(f\"- Model: {model_name}\")\n",
    "print(\n",
    "    f\"- Context Lengths: {len(context_lengths)}, Min: {min(context_lengths)}, Max: {max(context_lengths)}\"\n",
    ")\n",
    "print(\n",
    "    f\"- Document Depths: {len(document_depth_percents)}, Min: {min(document_depth_percents)}%, Max: {max(document_depth_percents)}%\"\n",
    ")\n",
    "print(f\"- Needle: {needle.strip()}\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c366b54a-5a42-4589-b1ca-392e9da55612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "0\n",
      "4 28\n",
      "4 28\n",
      "8 24\n",
      "8 24\n",
      "8 24\n",
      "16 16\n",
      "8 24\n",
      "16 16\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "8 24\n",
      "24 8\n",
      "20 12\n",
      "20 12\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "12 20\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "16 16\n",
      "24 8\n",
      "20 12\n",
      "32 0\n",
      "8 24\n",
      "4 28\n",
      "4 28\n",
      "8 24\n",
      "8 24\n",
      "8 24\n",
      "16 16\n",
      "8 24\n",
      "16 16\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "8 24\n",
      "24 8\n",
      "20 12\n",
      "20 12\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "12 20\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "16 16\n",
      "24 8\n",
      "20 12\n",
      "32 0\n",
      "8 24\n",
      "4 28\n",
      "4 28\n",
      "8 24\n",
      "8 24\n",
      "8 24\n",
      "16 16\n",
      "8 24\n",
      "16 16\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "8 24\n",
      "24 8\n",
      "20 12\n",
      "20 12\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "12 20\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "16 16\n",
      "24 8\n",
      "20 12\n",
      "32 0\n",
      "8 24\n",
      "4 28\n",
      "4 28\n",
      "8 24\n",
      "8 24\n",
      "8 24\n",
      "16 16\n",
      "8 24\n",
      "16 16\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "8 24\n",
      "24 8\n",
      "20 12\n",
      "20 12\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "20 12\n",
      "24 8\n",
      "12 20\n",
      "12 20\n",
      "24 8\n",
      "16 16\n",
      "20 12\n",
      "12 20\n",
      "16 16\n",
      "24 8\n",
      "20 12\n",
      "32 0\n",
      "8 24\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]] 0.8984375\n",
      "0 32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 0 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 231\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth_percent \u001b[38;5;129;01min\u001b[39;00m document_depth_percents:\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mprint\u001b[39m(depth_percent)\n\u001b[0;32m--> 231\u001b[0m     task \u001b[38;5;241m=\u001b[39m \u001b[43mbound_evaluate_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_percent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 146\u001b[0m, in \u001b[0;36mbound_evaluate_and_log\u001b[0;34m(context_length, depth_percent)\u001b[0m\n\u001b[1;32m    144\u001b[0m generated_content \u001b[38;5;241m=\u001b[39m [pred_token_idx\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_token_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values\n\u001b[1;32m    153\u001b[0m     pred_token_idx \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/duo-attention/duo_attn/patch/tuple_kv_cache.py:268\u001b[0m, in \u001b[0;36mold_llama_for_causal_lm_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    263\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    264\u001b[0m     return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    265\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/duo-attention/duo_attn/patch/tuple_kv_cache.py:409\u001b[0m, in \u001b[0;36mold_llama_model_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    405\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    407\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/duo-attention/duo_attn/patch/tuple_kv_cache.py:464\u001b[0m, in \u001b[0;36mold_llama_decoder_layer_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/duo-attention/duo_attn/patch/llama.py:210\u001b[0m, in \u001b[0;36mllama_duo_attention_forward_one_way_reordered\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m past_full_key_states \u001b[38;5;241m=\u001b[39m past_full_KV[:bsz]\n\u001b[1;32m    208\u001b[0m past_full_value_states \u001b[38;5;241m=\u001b[39m past_full_KV[bsz:]\n\u001b[0;32m--> 210\u001b[0m full_key_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_full_key_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m full_value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    212\u001b[0m     [past_full_value_states, full_value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m past_streaming_key_states \u001b[38;5;241m=\u001b[39m past_streaming_KV[:bsz]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 0 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "def get_context_length_in_tokens(context):\n",
    "    return len(enc.encode(context))\n",
    "    \n",
    "def read_context_files():\n",
    "    context = \"\"\n",
    "    max_context_length = max(context_lengths)\n",
    "    while get_context_length_in_tokens(context) < max_context_length:\n",
    "        for file in glob.glob(f\"{haystack_dir}/*.txt\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                context += f.read()\n",
    "    return context\n",
    "    \n",
    "def get_tokens_from_context(context):\n",
    "    return enc.encode(context)\n",
    "    \n",
    "def decode_tokens(tokens, context_length=None):\n",
    "    return enc.decode(tokens[:context_length], skip_special_tokens=True)\n",
    "    \n",
    "def encode_and_trim(context, context_length):\n",
    "    tokens = get_tokens_from_context(context)\n",
    "    if len(tokens) > context_length:\n",
    "        context = decode_tokens(tokens, context_length)\n",
    "    return context\n",
    "\n",
    "def encode_text_to_tokens(text):\n",
    "    return enc.encode(text, add_special_tokens=False)\n",
    "    \n",
    "def insert_needle(context, depth_percent, context_length):\n",
    "    tokens_needle = encode_text_to_tokens(needle)\n",
    "    tokens_context = encode_text_to_tokens(context)\n",
    "\n",
    "    # Reducing the context length by 150 buffer. This is to account for system message, the user question, and response.\n",
    "    context_length -= final_context_length_buffer\n",
    "\n",
    "    # If your context + needle are longer than the context length (which it will be), then reduce tokens from the context by the needle length\n",
    "    if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "        tokens_context = tokens_context[: context_length - len(tokens_needle)]\n",
    "\n",
    "    if depth_percent == 100:\n",
    "        # If your depth percent is 100 (which means your needle is the last thing in the doc), throw it at the end\n",
    "        tokens_new_context = tokens_context + tokens_needle\n",
    "    else:\n",
    "        insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "\n",
    "        tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "        tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "    # Convert back to a string and return it\n",
    "    new_context = decode_tokens(tokens_new_context)\n",
    "    return new_context\n",
    "    \n",
    "def generate_context(context_length, depth_percent):\n",
    "    # Load up tiktoken so we navigate tokens more easily\n",
    "\n",
    "    # Get your Paul Graham files loaded into a string\n",
    "    context = read_context_files()\n",
    "\n",
    "    # Truncate the Paul Graham essays to the context length you desire\n",
    "    context = encode_and_trim(context, context_length)\n",
    "\n",
    "    # Insert your random statement according to your depth percent\n",
    "    context = insert_needle(context, depth_percent, context_length)\n",
    "\n",
    "    return context\n",
    "    \n",
    "def generate_prompt(context):\n",
    "    test_format = f\"<|im_start|> This is a very long story book: <book> {context} </book>.\\n\\nQuestion: Based on the content of the book, {retrieval_question}\"\n",
    "    return test_format\n",
    "    \n",
    "def bound_evaluate_and_log(context_length, depth_percent):\n",
    "    # Go generate the required length context and place your needle statement in\n",
    "    context = generate_context(context_length, depth_percent)\n",
    "    \n",
    "    # Prepare your message to send to the model you're going to evaluate\n",
    "    prompt = generate_prompt(context)\n",
    "\n",
    "    generated_prompt = prompt\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    # Simulate multiround conversation\n",
    "    prompt = enc(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    prompt_input_ids = prompt[\"input_ids\"].to(model_to_test.device)\n",
    "\n",
    "    simulation_start_idx = prompt_input_ids.size(1) - simulation_length\n",
    "\n",
    "    # question_input_ids = prompt_input_ids[:, simulation_start_idx:]\n",
    "    # prompt_input_ids = prompt_input_ids[:, :simulation_start_idx]\n",
    "    # print(prompt_input_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if prefilling_chunk_size is not None:\n",
    "            past_key_values = None\n",
    "            for i in range(\n",
    "                0, prompt_input_ids.size(1), prefilling_chunk_size\n",
    "            ):\n",
    "                chunk = prompt_input_ids[:, i : i + prefilling_chunk_size]\n",
    "                output = model_to_test(\n",
    "                    input_ids=chunk,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                past_key_values = output.past_key_values\n",
    "        else:\n",
    "            output = model_to_test(\n",
    "                input_ids=prompt_input_ids, past_key_values=None, use_cache=True\n",
    "            )\n",
    "            past_key_values = output.past_key_values\n",
    "\n",
    "        # for input_id in question_input_ids[0]:\n",
    "        #     output = model_to_test(\n",
    "        #         input_ids=input_id.unsqueeze(0).unsqueeze(0),\n",
    "        #         past_key_values=past_key_values,\n",
    "        #         use_cache=True,\n",
    "        #     )\n",
    "        #     past_key_values = output.past_key_values\n",
    "\n",
    "        \n",
    "        # attn_heads_after, sparsity = sparsify_attention_heads(attn_heads, sparsity=0.90)\n",
    "        # print(attn_heads_after, sparsity)\n",
    "\n",
    "        # device = next(model_to_test.parameters()).device\n",
    "        # dtype = next(model_to_test.parameters()).dtype\n",
    "        # for idx, layer in enumerate(model_to_test.model.layers):\n",
    "        #     module = layer.self_attn\n",
    "        #     layer_full_attention_heads = torch.tensor(\n",
    "        #         attn_heads_after[idx], device=device, dtype=dtype\n",
    "        #     )\n",
    "            \n",
    "        #     module.full_attn_head_mask = layer_full_attention_heads > 0.5\n",
    "        #     module.num_full_attn_head = module.full_attn_head_mask.sum().item()\n",
    "        #     module.num_streaming_attn_head = (\n",
    "        #         module.num_key_value_heads - module.num_full_attn_head\n",
    "        #     )\n",
    "    \n",
    "        #     module.num_full_query_head = module.num_full_attn_head * module.num_key_value_groups\n",
    "        #     module.num_streaming_query_head = module.num_heads - module.num_full_query_head\n",
    "        #     # print(module, module.num_full_query_head)\n",
    "\n",
    "        pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        generated_content = [pred_token_idx.item()]\n",
    "        for _ in range(50):\n",
    "            outputs = model_to_test(\n",
    "                input_ids=pred_token_idx,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "            pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "            generated_content += [pred_token_idx.item()]\n",
    "            if pred_token_idx.item() in eos_token_ids:\n",
    "                break\n",
    "\n",
    "    response = enc.decode(generated_content, skip_special_tokens=True).strip()\n",
    "    \n",
    "    test_end_time = time.time()\n",
    "    test_elapsed_time = test_end_time - test_start_time\n",
    "    score = scorer.score(needle, response)[\"rouge1\"].fmeasure * 10\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "        \"model\": model_to_test_description,\n",
    "        \"context_length\": int(context_length),\n",
    "        \"depth_percent\": float(depth_percent),\n",
    "        \"needle\": needle,\n",
    "        \"model_response\": response,\n",
    "        \"score\": score,\n",
    "        \"test_duration_seconds\": test_elapsed_time,\n",
    "        \"test_timestamp_utc\": datetime.now(timezone.utc).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S%z\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    testing_results.append(results)\n",
    "    print(f\"-- Test Summary -- \")\n",
    "    print(f\"Duration: {test_elapsed_time:.1f} seconds\")\n",
    "    print(f\"Context: {context_length} tokens\")\n",
    "    print(f\"Depth: {depth_percent}%\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "    model_version = model_name.split(\"/\")[-1]\n",
    "    context_file_location = f'{model_version.replace(\".\", \"_\")}_len_{context_length}_depth_{int(depth_percent*100)}'\n",
    "\n",
    "    results[\"file_name\"] = context_file_location\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"contexts\"):\n",
    "        os.makedirs(\"contexts\")\n",
    "\n",
    "    if not os.path.exists(f\"contexts/{model_version}\"):\n",
    "        os.makedirs(f\"contexts/{model_version}\")\n",
    "\n",
    "    with open(\n",
    "        f\"contexts/{model_version}/{context_file_location}_context.txt\",\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        f.write(context)\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    if not os.path.exists(f\"results/{model_version}\"):\n",
    "        os.makedirs(f\"results/{model_version}\")\n",
    "\n",
    "    # Save the result to file for retesting\n",
    "    p = f\"results/{model_version}/{context_file_location}_results.json\"\n",
    "    print(\"Writing at %s\" % p)\n",
    "    print(p)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return None, generated_prompt\n",
    "\n",
    "s_len = 1\n",
    "e_len = pretrained_len\n",
    "tasks = []\n",
    "for context_length in context_lengths:\n",
    "    print(context_length)\n",
    "    if context_length < s_len or context_length > e_len:\n",
    "        continue\n",
    "    for depth_percent in document_depth_percents:\n",
    "        print(depth_percent)\n",
    "        task = bound_evaluate_and_log(context_length, depth_percent)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a2f8-9020-4828-9f4f-84adafac2cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
