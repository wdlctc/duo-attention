{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34759c07-f3e3-481b-a2b7-3268b50a09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is adapted from \n",
    "https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031704f1-2b4f-4bb3-b28f-4816671c64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from models/Llama-3-8B-Instruct-Gradient-1048k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9562197757434d1d888946e591bd9951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_lengths_min=120000\n",
    "context_lengths_max=1048000\n",
    "context_lengths_num_intervals=40\n",
    "pretrained_len=1048000\n",
    "sparsity=0.5\n",
    "document_depth_percent_min=0\n",
    "document_depth_percent_max=100\n",
    "document_depth_percent_intervals=10\n",
    "document_depth_percent_interval_type=\"linear\"\n",
    "final_context_length_buffer=200\n",
    "simulation_length=50\n",
    "prefilling_chunk_size=32000\n",
    "prefilling_chunk_size = None\n",
    "\n",
    "needle=\"\\n\\nRemember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\\n\"\n",
    "retrieval_question=\"what is the best thing to do in San Francisco?\\n\\nAnswer: The best thing to do in San Francisco is\"\n",
    "haystack_dir=\"eval/needle/PaulGrahamEssays\"\n",
    "testing_results = []\n",
    "\n",
    "context_lengths = np.round(\n",
    "    np.linspace(\n",
    "        context_lengths_min,\n",
    "        context_lengths_max,\n",
    "        num=context_lengths_num_intervals,\n",
    "        endpoint=True,\n",
    "    )\n",
    ").astype(int)\n",
    "\n",
    "if document_depth_percent_interval_type == \"linear\":\n",
    "    document_depth_percents = np.round(\n",
    "        np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            num=document_depth_percent_intervals,\n",
    "            endpoint=True,\n",
    "        )\n",
    "    ).astype(int)\n",
    "elif document_depth_percent_interval_type == \"sigmoid\":\n",
    "    document_depth_percents = [\n",
    "        logistic(x)\n",
    "        for x in np.linspace(\n",
    "            document_depth_percent_min,\n",
    "            document_depth_percent_max,\n",
    "            document_depth_percent_intervals,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "model_name = \"models/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "model_to_test_description = model_name\n",
    "enc = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "if enc.pad_token_id is None:\n",
    "    if enc.eos_token_id is not None:\n",
    "        enc.pad_token_id = enc.eos_token_id\n",
    "    else:\n",
    "        enc.pad_token_id = 0\n",
    "print(\"Loading from %s\" % model_name)\n",
    "\n",
    "model_to_test = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eff818-840f-41de-b18c-54a256b11f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8)\n",
      "128\n",
      "256\n",
      "[[8.59375000e-01 6.52343750e-01 1.00000000e+00 3.39843750e-01\n",
      "  0.00000000e+00 6.79687500e-01 3.49609375e-01 2.73437500e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 8.24218750e-01 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.91406250e-01 0.00000000e+00 9.60937500e-01\n",
      "  7.30468750e-01 9.84375000e-01 9.64843750e-01 0.00000000e+00]\n",
      " [7.85156250e-01 2.30073929e-05 3.83853912e-05 0.00000000e+00\n",
      "  8.71093750e-01 9.76562500e-01 1.00000000e+00 4.21875000e-01]\n",
      " [0.00000000e+00 0.00000000e+00 1.17675781e-01 8.63281250e-01\n",
      "  1.00000000e+00 9.88281250e-01 8.35937500e-01 2.61306763e-04]\n",
      " [7.89062500e-01 9.84375000e-01 1.00000000e+00 5.82031250e-01\n",
      "  1.00000000e+00 9.45312500e-01 9.52148438e-02 1.00000000e+00]\n",
      " [5.00000000e-01 8.63281250e-01 7.03125000e-01 7.18750000e-01\n",
      "  9.10156250e-01 1.00000000e+00 1.00000000e+00 8.94531250e-01]\n",
      " [1.00000000e+00 8.32031250e-01 1.44042969e-02 1.00000000e+00\n",
      "  6.10351562e-05 1.00000000e+00 1.00000000e+00 3.41796875e-01]\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.54296875e-01\n",
      "  1.00000000e+00 4.88281250e-01 9.72656250e-01 1.00000000e+00]\n",
      " [9.14062500e-01 7.65625000e-01 4.88281250e-01 1.00000000e+00\n",
      "  8.39843750e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 8.39843750e-01 8.78906250e-01 9.84375000e-01\n",
      "  1.00000000e+00 5.66406250e-01 1.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 8.08593750e-01 9.57031250e-01\n",
      "  9.45312500e-01 8.63281250e-01 7.22656250e-02 1.00000000e+00]\n",
      " [1.00000000e+00 4.83398438e-02 7.26562500e-01 8.71093750e-01\n",
      "  9.37500000e-01 1.00000000e+00 3.96728516e-04 4.49218750e-01]\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 9.33593750e-01\n",
      "  1.00000000e+00 8.00781250e-01 1.00000000e+00 1.00000000e+00]\n",
      " [6.17187500e-01 1.00000000e+00 5.07812500e-01 9.49218750e-01\n",
      "  9.80468750e-01 9.64843750e-01 9.72656250e-01 1.00000000e+00]\n",
      " [1.00000000e+00 6.52343750e-01 1.00000000e+00 2.34375000e-01\n",
      "  1.00000000e+00 8.47656250e-01 1.00000000e+00 1.00000000e+00]\n",
      " [9.92187500e-01 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  9.33593750e-01 1.00000000e+00 1.00000000e+00 7.26562500e-01]\n",
      " [1.00000000e+00 1.00000000e+00 9.33593750e-01 1.00000000e+00\n",
      "  7.73437500e-01 9.64843750e-01 1.00000000e+00 9.80468750e-01]\n",
      " [5.19531250e-01 9.96093750e-01 7.96875000e-01 1.00000000e+00\n",
      "  9.17968750e-01 9.64843750e-01 1.39236450e-04 8.51562500e-01]\n",
      " [1.00000000e+00 7.61718750e-01 1.00000000e+00 1.00000000e+00\n",
      "  1.48010254e-03 9.80468750e-01 5.85937500e-01 1.00000000e+00]\n",
      " [9.88281250e-01 0.00000000e+00 1.00000000e+00 9.88281250e-01\n",
      "  1.00000000e+00 9.92187500e-01 1.00000000e+00 2.71484375e-01]\n",
      " [1.00000000e+00 9.10156250e-01 9.72656250e-01 1.00000000e+00\n",
      "  9.25781250e-01 9.57031250e-01 8.08593750e-01 5.78125000e-01]\n",
      " [9.29687500e-01 9.53125000e-01 8.39843750e-01 9.88281250e-01\n",
      "  1.00000000e+00 7.10937500e-01 7.89062500e-01 9.64843750e-01]\n",
      " [1.00000000e+00 1.00000000e+00 3.12805176e-04 9.72656250e-01\n",
      "  3.90625000e-03 9.84375000e-01 9.88281250e-01 1.00000000e+00]\n",
      " [1.00000000e+00 5.11718750e-01 0.00000000e+00 9.37500000e-01\n",
      "  9.76562500e-01 9.88281250e-01 9.92187500e-01 7.34375000e-01]\n",
      " [9.68750000e-01 9.92187500e-01 7.62939453e-05 9.96093750e-01\n",
      "  9.60937500e-01 1.00000000e+00 0.00000000e+00 6.38961792e-05]\n",
      " [1.00000000e+00 9.10156250e-01 6.83593750e-01 1.00000000e+00\n",
      "  9.21875000e-01 8.47656250e-01 8.90625000e-01 9.96093750e-01]\n",
      " [5.86509705e-05 1.00000000e+00 8.90625000e-01 9.53125000e-01\n",
      "  9.45312500e-01 9.96093750e-01 1.00000000e+00 9.88281250e-01]\n",
      " [9.96093750e-01 9.72656250e-01 9.17968750e-01 9.84375000e-01\n",
      "  1.00000000e+00 9.88281250e-01 1.00000000e+00 9.49218750e-01]\n",
      " [0.00000000e+00 1.11579895e-04 1.00000000e+00 1.37329102e-04\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      " [9.92187500e-01 1.00000000e+00 9.96093750e-01 9.96093750e-01\n",
      "  1.00000000e+00 1.00000000e+00 9.96093750e-01 9.96093750e-01]\n",
      " [9.92187500e-01 8.90625000e-01 9.41406250e-01 7.92968750e-01\n",
      "  9.29687500e-01 9.64843750e-01 9.29687500e-01 9.60937500e-01]] 0.5\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0.]] 0.5\n"
     ]
    }
   ],
   "source": [
    "from duo_attn.utils import load_attn_pattern, sparsify_attention_heads\n",
    "from duo_attn.patch import enable_duo_attention_eval\n",
    "\n",
    "# Load the attention pattern\n",
    "attn_heads, sink_size, recent_size = load_attn_pattern(\n",
    "    \"attn_patterns/Llama-3-8B-Instruct-Gradient-1048k/lr=0.02-reg=0.05-ctx=1000_32000-multi_passkey10\"\n",
    ")\n",
    "model = model_to_test\n",
    "print(attn_heads.shape)\n",
    "print(sink_size)\n",
    "print(recent_size)\n",
    "\n",
    "# Sparsify attention heads\n",
    "\n",
    "print(attn_heads, sparsity)\n",
    "attn_heads, sparsity = sparsify_attention_heads(attn_heads, sparsity=0.5)\n",
    "\n",
    "print(attn_heads, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cbc0ad-b07a-4a25-9446-66b6b1e26f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False False]\n",
      "[False False False  True False False False False]\n",
      "[False False False False False  True  True False]\n",
      "[False False False False False  True  True False]\n",
      "[False False False False  True  True False False]\n",
      "[False  True  True False  True False False  True]\n",
      "[False False False False False  True  True False]\n",
      "[ True False False  True False  True  True False]\n",
      "[ True  True  True False  True False  True  True]\n",
      "[False False False  True False  True  True  True]\n",
      "[ True False False  True  True False  True  True]\n",
      "[ True  True False False False False False  True]\n",
      "[ True False False False False  True False False]\n",
      "[ True  True  True False  True False  True  True]\n",
      "[False  True False False  True  True  True  True]\n",
      "[ True False  True False  True False  True  True]\n",
      "[ True  True  True False False  True  True False]\n",
      "[ True  True False  True False  True  True  True]\n",
      "[False  True False  True False  True False False]\n",
      "[ True False  True  True False  True False  True]\n",
      "[ True False  True  True  True  True  True False]\n",
      "[ True False  True  True False False False False]\n",
      "[False False False  True  True False False  True]\n",
      "[ True  True False  True False  True  True  True]\n",
      "[ True False False False  True  True  True False]\n",
      "[ True  True False  True  True  True False False]\n",
      "[ True False False  True False False False  True]\n",
      "[False  True False False False  True  True  True]\n",
      "[ True  True False  True  True  True  True False]\n",
      "[False False  True False  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True]\n",
      "[ True False False False False  True False False]\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    logger,\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    "    LlamaSdpaAttention,\n",
    "    LlamaFlashAttention2,\n",
    ")\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "import types\n",
    "from transformers.modeling_flash_attention_utils  import _flash_attention_forward\n",
    "from flash_attn import flash_attn_func\n",
    "\n",
    "def LlamaAttention_fast_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") :\n",
    "    output_attentions = False\n",
    "\n",
    "    bsz, q_len, hd = hidden_states.size()\n",
    "    chunk_size = hd // self.num_key_value_heads\n",
    "    num_heads = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "    if not hasattr(self, 'q_proj_list'):\n",
    "        self.q_proj_list = list((self.q_proj.weight.split(self.head_dim * num_heads, dim=0)))\n",
    "        # self.q_proj.weight.data.storage().resize_(0)\n",
    "    if not hasattr(self, 'k_proj_list'):\n",
    "        self.k_proj_list = list((self.k_proj.weight.split(self.head_dim, dim=0)))\n",
    "        # self.k_proj.weight.data.storage().resize_(0)\n",
    "    if not hasattr(self, 'v_proj_list'):\n",
    "        self.v_proj_list = list((self.v_proj.weight.split(self.head_dim, dim=0)))\n",
    "        # self.v_proj.weight.data.storage().resize_(0)\n",
    "\n",
    "\n",
    "    attn_output_list = [None for _ in range((self.num_key_value_heads))]\n",
    "    \n",
    "    for i in range(self.num_key_value_heads):\n",
    "        bsz, q_len, hd = hidden_states.size()\n",
    "\n",
    "        self.q_proj.weight.data = self.q_proj_list[i].data\n",
    "        self.k_proj.weight.data = self.k_proj_list[i].data\n",
    "        self.v_proj.weight.data = self.v_proj_list[i].data\n",
    "\n",
    "        # print(hidden_states.shape, self.q_proj.weight.shape)\n",
    "        \n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Flash attention requires the input to have the shape\n",
    "        # batch_size x seq_length x head_dim x hidden_dim\n",
    "        # therefore we just need to keep the original shape\n",
    "        query_states = query_states.view(bsz, q_len, num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "    \n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position, 'full_head': self.full_attn_head_mask[i]}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx + i, cache_kwargs)\n",
    "    \n",
    "        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
    "        # to be able to avoid many of these transpose/reshape/view.\n",
    "        query_states = query_states.transpose(1, 2)\n",
    "        key_states = key_states.transpose(1, 2)\n",
    "        value_states = value_states.transpose(1, 2)\n",
    "    \n",
    "        dropout_rate = self.attention_dropout if self.training else 0.0\n",
    "    \n",
    "        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
    "        # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
    "        # cast them back in the correct dtype just to be sure everything works as expected.\n",
    "        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
    "        # in fp32. (LlamaRMSNorm handles it correctly)\n",
    "    \n",
    "        input_dtype = query_states.dtype\n",
    "        if input_dtype == torch.float32:\n",
    "            if torch.is_autocast_enabled():\n",
    "                target_dtype = torch.get_autocast_gpu_dtype()\n",
    "            # Handle the case where the model is quantized\n",
    "            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n",
    "                target_dtype = self.config._pre_quantization_dtype\n",
    "            else:\n",
    "                target_dtype = self.q_proj.weight.dtype\n",
    "    \n",
    "            logger.warning_once(\n",
    "                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n",
    "                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n",
    "                f\" {target_dtype}.\"\n",
    "            )\n",
    "    \n",
    "            query_states = query_states.to(target_dtype)\n",
    "            key_states = key_states.to(target_dtype)\n",
    "            value_states = value_states.to(target_dtype)\n",
    "    \n",
    "        attn_output = flash_attn_func(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            causal=True,\n",
    "            dropout_p=0.0,\n",
    "        )\n",
    "    \n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n",
    "        attn_output_list[i] = attn_output\n",
    "        \n",
    "    attn_output = torch.cat(attn_output_list, dim=-1)\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    if not output_attentions:\n",
    "        attn_weights = None\n",
    "\n",
    "    return attn_output, attn_weights, past_key_value\n",
    "\n",
    "full_attention_heads = attn_heads\n",
    "LlamaFlashAttention2.forward = LlamaAttention_fast_forward\n",
    "layer_idx = 0\n",
    "for idx, layer in enumerate(model.model.layers):\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    module = layer.self_attn\n",
    "    module.layer_idx = layer_idx\n",
    "    layer_idx += module.num_key_value_heads\n",
    "\n",
    "    # print(full_attention_heads)\n",
    "    # layer_full_attention_heads = torch.tensor(\n",
    "    #     full_attention_heads[idx], device=device, dtype=dtype\n",
    "    # )\n",
    "    # threshold = np.quantile(full_attention_heads, 0.5)\n",
    "    module.full_attn_head_mask = full_attention_heads[idx] >= 0.5\n",
    "    module.num_full_attn_head = module.full_attn_head_mask.sum().item()\n",
    "\n",
    "    print(module.full_attn_head_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5aee80a-ff7c-405d-ac90-3782a3095dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DynamicCache(Cache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Prepare a cache class and pass it to model's forward\n",
    "        >>> past_key_values = DynamicCache()\n",
    "        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n",
    "        >>> outputs.past_key_values # access cache filled with key/values from generation\n",
    "        DynamicCache()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n",
    "        sequence length.\n",
    "        \"\"\"\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n",
    "        keys and values\n",
    "        \"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n",
    "        to the number of layers in the model.\n",
    "        \"\"\"\n",
    "        return len(self.key_cache)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            # There may be skipped layers, fill them with empty lists\n",
    "            for _ in range(len(self.key_cache), layer_idx):\n",
    "                self.key_cache.append([])\n",
    "                self.value_cache.append([])\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
    "            self.key_cache[layer_idx] = key_states\n",
    "            self.value_cache[layer_idx] = value_states\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        \n",
    "        if cache_kwargs['full_head'] == False:\n",
    "            key, value = self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "            incoming_kv_seq_len = self.key_cache[layer_idx].shape[2]\n",
    "            if incoming_kv_seq_len > self.sink_size + self.recent_size:\n",
    "                sink_key_states = self.key_cache[layer_idx][:, :, : self.sink_size, :].clone()\n",
    "                recent_key_states = self.key_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ].clone()\n",
    "                self.key_cache[layer_idx] = torch.cat([sink_key_states, recent_key_states], dim=-2)\n",
    "\n",
    "                sink_value_states = self.value_cache[layer_idx][:, :, : self.sink_size, :].clone()\n",
    "                recent_value_states = self.value_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ].clone()\n",
    "                self.value_cache[layer_idx] = torch.cat([sink_value_states, recent_value_states], dim=-2)\n",
    "        else:\n",
    "            key, value = self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "        return key, value\n",
    "\n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n",
    "        # TODO: deprecate this function in favor of `cache_position`\n",
    "        is_empty_layer = (\n",
    "            len(self.key_cache) == 0  # no cache in any layer\n",
    "            or len(self.key_cache) <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n",
    "            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
    "        )\n",
    "        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n",
    "        return layer_seq_length\n",
    "\n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format. Used for\n",
    "        backward compatibility.\"\"\"\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "        return legacy_cache\n",
    "\n",
    "    @classmethod\n",
    "    def from_legacy_cache(\n",
    "        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, num_hidden_layers: int = None\n",
    "    ) -> \"DynamicCache\":\n",
    "        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`. Used for\n",
    "        backward compatibility.\"\"\"\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(key_states, value_states, layer_idx)\n",
    "        return cache\n",
    "\n",
    "    def crop(self, max_length: int):\n",
    "        \"\"\"Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n",
    "        negative to remove `max_length` tokens. This is used in assisted decoding and contrastive search.\"\"\"\n",
    "        # In case it is negative\n",
    "        if max_length < 0:\n",
    "            max_length = self.get_seq_length() - abs(max_length)\n",
    "\n",
    "        if self.get_seq_length() <= max_length:\n",
    "            return\n",
    "\n",
    "        self._seen_tokens = max_length\n",
    "        for idx in range(len(self.key_cache)):\n",
    "            if self.key_cache[idx] != []:\n",
    "                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n",
    "                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n",
    "\n",
    "    def batch_split(\n",
    "        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n",
    "    ) -> List[\"DynamicCache\"]:\n",
    "        \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n",
    "        `_split_model_inputs()` in `generation.utils`\"\"\"\n",
    "        out = []\n",
    "        for i in range(0, full_batch_size, split_size):\n",
    "            current_split = DynamicCache()\n",
    "            current_split._seen_tokens = self._seen_tokens\n",
    "            current_split.key_cache = [tensor[i : i + split_size] for tensor in self.key_cache]\n",
    "            current_split.value_cache = [tensor[i : i + split_size] for tensor in self.value_cache]\n",
    "            out.append(current_split)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int = None) -> \"DynamicCache\":\n",
    "        \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n",
    "        `generation.utils`\"\"\"\n",
    "        cache = cls()\n",
    "        for idx in range(len(splits[0])):\n",
    "            key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n",
    "            value_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n",
    "            if key_cache != []:\n",
    "                layer_keys = torch.cat(key_cache, dim=0)\n",
    "                layer_values = torch.cat(value_cache, dim=0)\n",
    "                cache.update(layer_keys, layer_values, idx)\n",
    "        return cache\n",
    "\n",
    "    def batch_repeat_interleave(self, repeats: int):\n",
    "        \"\"\"Repeat the cache `repeats` times in the batch dimension. Used in contrastive search.\"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
    "\n",
    "    def batch_select_indices(self, indices: torch.Tensor):\n",
    "        \"\"\"Only keep the `indices` in the batch dimension of the cache. Used in contrastive search.\"\"\"\n",
    "        for layer_idx in range(len(self)):\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx][indices, ...]\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ea764d-df18-4e50-97e7-6c71de42a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OffloadedCache(DynamicCache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Prepare a cache class and pass it to model's forward\n",
    "        >>> past_key_values = DynamicCache()\n",
    "        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n",
    "        >>> outputs.past_key_values # access cache filled with key/values from generation\n",
    "        DynamicCache()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"OffloadedCache can only be used with a GPU\")\n",
    "        super().__init__()\n",
    "        self.original_device = []\n",
    "        self.prefetch_stream = torch.cuda.Stream()\n",
    "        self.beam_idx = None  # used to delay beam search operations\n",
    "\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        \n",
    "        self.offload_key_cache: List[torch.Tensor] = []\n",
    "        self.offload_value_cache: List[torch.Tensor] = []\n",
    "\n",
    "        self.id_type_list = []\n",
    "        self.real_id_dict = {}\n",
    "\n",
    "    def prefetch_layer(self, layer_idx: int):\n",
    "        \"Starts prefetching the next layer cache\"\n",
    "        if layer_idx < len(self.offload_key_cache):\n",
    "            with torch.cuda.stream(self.prefetch_stream):\n",
    "                # Prefetch next layer tensors to GPU\n",
    "                device = self.original_device[layer_idx]\n",
    "                self.offload_key_cache[layer_idx] = self.offload_key_cache[layer_idx].to(device, non_blocking=True)\n",
    "                self.offload_value_cache[layer_idx] = self.offload_value_cache[layer_idx].to(device, non_blocking=True)\n",
    "    \n",
    "    def evict_previous_layer(self, layer_idx: int):\n",
    "        \"Moves the previous layer cache to the CPU\"\n",
    "        if len(self.offload_key_cache) > 2:\n",
    "            # We do it on the default stream so it occurs after all earlier computations on these tensors are done\n",
    "            prev_layer_idx = (layer_idx - 1) % len(self.offload_key_cache)\n",
    "            self.offload_key_cache[prev_layer_idx] = self.offload_key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            self.offload_value_cache[prev_layer_idx] = self.offload_value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            \n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n",
    "        sequence length.\n",
    "        \"\"\"\n",
    "        if layer_idx < len(self.id_type_list):\n",
    "            if self.id_type_list[layer_idx] == True:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "            else:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                # Evict the previous layer if necessary\n",
    "                torch.cuda.current_stream().synchronize()\n",
    "                self.evict_previous_layer(layer_idx)\n",
    "                # Load current layer cache to its original device if not already there\n",
    "                original_device = self.original_device[layer_idx]\n",
    "                self.prefetch_stream.synchronize()\n",
    "                key_tensor = self.offload_key_cache[layer_idx]\n",
    "                value_tensor = self.offload_value_cache[layer_idx]\n",
    "                # Now deal with beam search ops which were delayed\n",
    "                if self.beam_idx is not None:\n",
    "                    self.beam_idx = self.beam_idx.to(original_device)\n",
    "                    key_tensor = key_tensor.index_select(0, self.beam_idx)\n",
    "                    value_tensor = value_tensor.index_select(0, self.beam_idx)\n",
    "                # Prefetch the next layer\n",
    "                self.prefetch_layer((layer_idx + 1) % len(self))\n",
    "                return (key_tensor, value_tensor)\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        \"\"\"Saves the beam indices and reorders the cache when the tensor is back to its device.\"\"\"\n",
    "        # We delay this operation until the tensors are back to their original\n",
    "        # device because performing torch.index_select on the CPU is very slow\n",
    "        del self.beam_idx\n",
    "        self.beam_idx = beam_idx.clone()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        is_stream_head = (cache_kwargs['full_head'] == False)\n",
    "        if len(self.id_type_list) <= layer_idx:\n",
    "            self.id_type_list.append(is_stream_head)\n",
    "\n",
    "        original_layer_layer_idx = layer_idx\n",
    "\n",
    "        # print(layer_idx, cache_kwargs['full_head'])\n",
    "\n",
    "        if is_stream_head:\n",
    "            if layer_idx in self.real_id_dict:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            else:\n",
    "                self.real_id_dict[layer_idx] = len(self.key_cache)\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            if len(self.key_cache) <= layer_idx:\n",
    "                # There may be skipped layers, fill them with empty lists\n",
    "                for _ in range(len(self.key_cache), layer_idx):\n",
    "                    self.key_cache.append([])\n",
    "                    self.value_cache.append([])\n",
    "                self.key_cache.append(key_states)\n",
    "                self.value_cache.append(value_states)\n",
    "            elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
    "                self.key_cache[layer_idx] = key_states\n",
    "                self.value_cache[layer_idx] = value_states\n",
    "            else:\n",
    "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "                \n",
    "            key_tensor, value_tensor = self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "            incoming_kv_seq_len = self.key_cache[layer_idx].shape[2]\n",
    "            if incoming_kv_seq_len > self.sink_size + self.recent_size:\n",
    "                sink_key_states = self.key_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "                recent_key_states = self.key_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ]\n",
    "                self.key_cache[layer_idx] = torch.cat([sink_key_states, recent_key_states], dim=-2)\n",
    "\n",
    "                sink_value_states = self.value_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "                recent_value_states = self.value_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ]\n",
    "                self.value_cache[layer_idx] = torch.cat([sink_value_states, recent_value_states], dim=-2)\n",
    "        else:\n",
    "            if layer_idx in self.real_id_dict:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            else:\n",
    "                self.real_id_dict[layer_idx] = len(self.offload_key_cache)\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                \n",
    "            if len(self.offload_key_cache) < layer_idx:\n",
    "                raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n",
    "            elif len(self.offload_key_cache) == layer_idx:\n",
    "                self.offload_key_cache.append(key_states)\n",
    "                self.offload_value_cache.append(value_states)\n",
    "                self.original_device.append(key_states.device)\n",
    "                self.evict_previous_layer(layer_idx)\n",
    "                key_tensor, value_tensor = key_states, value_states\n",
    "            else:\n",
    "                key_tensor, value_tensor = self[original_layer_layer_idx]\n",
    "                self.offload_key_cache[layer_idx] = torch.cat([key_tensor, key_states], dim=-2)\n",
    "                self.offload_value_cache[layer_idx] = torch.cat([value_tensor, value_states], dim=-2)\n",
    "                key_tensor = self.offload_key_cache[layer_idx]\n",
    "                value_tensor = self.offload_value_cache[layer_idx]\n",
    "\n",
    "        return key_tensor, value_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525bd5fd-fa45-4387-8df7-fd34c3555603",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_to_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3117\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m         )\n\u001b[1;32m   3116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_to_test = model_to_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f72801-6cd7-4429-bb79-a933519e10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Starting Needle In A Haystack Testing...\")\n",
    "print(f\"- Model: {model_name}\")\n",
    "print(\n",
    "    f\"- Context Lengths: {len(context_lengths)}, Min: {min(context_lengths)}, Max: {max(context_lengths)}\"\n",
    ")\n",
    "print(\n",
    "    f\"- Document Depths: {len(document_depth_percents)}, Min: {min(document_depth_percents)}%, Max: {max(document_depth_percents)}%\"\n",
    ")\n",
    "print(f\"- Needle: {needle.strip()}\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366b54a-5a42-4589-b1ca-392e9da55612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_context_length_in_tokens(context):\n",
    "    return len(enc.encode(context))\n",
    "    \n",
    "def read_context_files():\n",
    "    context = \"\"\n",
    "    max_context_length = max(context_lengths)\n",
    "    while get_context_length_in_tokens(context) < max_context_length:\n",
    "        for file in glob.glob(f\"{haystack_dir}/*.txt\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                context += f.read()\n",
    "    return context\n",
    "    \n",
    "def get_tokens_from_context(context):\n",
    "    return enc.encode(context)\n",
    "    \n",
    "def decode_tokens(tokens, context_length=None):\n",
    "    return enc.decode(tokens[:context_length], skip_special_tokens=True)\n",
    "    \n",
    "def encode_and_trim(context, context_length):\n",
    "    tokens = get_tokens_from_context(context)\n",
    "    if len(tokens) > context_length:\n",
    "        context = decode_tokens(tokens, context_length)\n",
    "    return context\n",
    "\n",
    "def encode_text_to_tokens(text):\n",
    "    return enc.encode(text, add_special_tokens=False)\n",
    "    \n",
    "def insert_needle(context, depth_percent, context_length):\n",
    "    tokens_needle = encode_text_to_tokens(needle)\n",
    "    tokens_context = encode_text_to_tokens(context)\n",
    "\n",
    "    # Reducing the context length by 150 buffer. This is to account for system message, the user question, and response.\n",
    "    context_length -= final_context_length_buffer\n",
    "\n",
    "    # If your context + needle are longer than the context length (which it will be), then reduce tokens from the context by the needle length\n",
    "    if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "        tokens_context = tokens_context[: context_length - len(tokens_needle)]\n",
    "\n",
    "    if depth_percent == 100:\n",
    "        # If your depth percent is 100 (which means your needle is the last thing in the doc), throw it at the end\n",
    "        tokens_new_context = tokens_context + tokens_needle\n",
    "    else:\n",
    "        insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "\n",
    "        tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "        tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "    # Convert back to a string and return it\n",
    "    new_context = decode_tokens(tokens_new_context)\n",
    "    return new_context\n",
    "    \n",
    "def generate_context(context_length, depth_percent):\n",
    "    # Load up tiktoken so we navigate tokens more easily\n",
    "\n",
    "    # Get your Paul Graham files loaded into a string\n",
    "    context = read_context_files()\n",
    "\n",
    "    # Truncate the Paul Graham essays to the context length you desire\n",
    "    context = encode_and_trim(context, context_length)\n",
    "\n",
    "    # Insert your random statement according to your depth percent\n",
    "    context = insert_needle(context, depth_percent, context_length)\n",
    "\n",
    "    return context\n",
    "    \n",
    "def generate_prompt(context):\n",
    "    test_format = f\"<|im_start|> This is a very long story book: <book> {context} </book>.\\n\\nQuestion: Based on the content of the book, {retrieval_question}\"\n",
    "    return test_format\n",
    "    \n",
    "def bound_evaluate_and_log(context_length, depth_percent):\n",
    "    # Go generate the required length context and place your needle statement in\n",
    "    context = generate_context(context_length, depth_percent)\n",
    "    \n",
    "    # Prepare your message to send to the model you're going to evaluate\n",
    "    prompt = generate_prompt(context)\n",
    "\n",
    "    generated_prompt = prompt\n",
    "\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    # Simulate multiround conversation\n",
    "    prompt = enc(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    prompt_input_ids = prompt[\"input_ids\"].to(model_to_test.device)\n",
    "\n",
    "    # simulation_start_idx = prompt_input_ids.size(1) - simulation_length\n",
    "\n",
    "    # question_input_ids = prompt_input_ids[:, simulation_start_idx:]\n",
    "    # prompt_input_ids = prompt_input_ids[:, :simulation_start_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        past_key_values = OffloadedCache()\n",
    "        past_key_values.sink_size = 64\n",
    "        past_key_values.recent_size = 256\n",
    "        if prefilling_chunk_size is not None:\n",
    "            for i in range(\n",
    "                0, prompt_input_ids.size(1), prefilling_chunk_size\n",
    "            ):\n",
    "                chunk = prompt_input_ids[:, i : i + prefilling_chunk_size]\n",
    "                output = model_to_test(\n",
    "                    input_ids=chunk,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                    num_logits_to_keep=1\n",
    "                )\n",
    "                past_key_values = output.past_key_values\n",
    "        else:\n",
    "            output = model_to_test(\n",
    "                input_ids=prompt_input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1\n",
    "            )\n",
    "            past_key_values = output.past_key_values\n",
    "\n",
    "        # for input_id in question_input_ids[0]:\n",
    "        #     output = model_to_test(\n",
    "        #         input_ids=input_id.unsqueeze(0).unsqueeze(0),\n",
    "        #         past_key_values=past_key_values,\n",
    "        #         use_cache=True,\n",
    "        #     )\n",
    "        #     past_key_values = output.past_key_values\n",
    "\n",
    "        # for idx, layer in enumerate(model.model.layers):\n",
    "        #     device = next(model.parameters()).device\n",
    "        #     dtype = next(model.parameters()).dtype\n",
    "        #     module = layer.self_attn\n",
    "            \n",
    "        #     module.full_attn_head_mask = full_attention_heads[idx] >= 0.75\n",
    "        \n",
    "        pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        generated_content = [pred_token_idx.item()]\n",
    "        for _ in range(50):\n",
    "            outputs = model_to_test(\n",
    "                input_ids=pred_token_idx,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                num_logits_to_keep=1,\n",
    "            )\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "            pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "            generated_content += [pred_token_idx.item()]\n",
    "            if pred_token_idx.item() in eos_token_ids:\n",
    "                break\n",
    "\n",
    "    response = enc.decode(generated_content, skip_special_tokens=True).strip()\n",
    "    \n",
    "    test_end_time = time.time()\n",
    "    test_elapsed_time = test_end_time - test_start_time\n",
    "    score = scorer.score(needle, response)[\"rouge1\"].fmeasure * 10\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "        \"model\": model_to_test_description,\n",
    "        \"context_length\": int(context_length),\n",
    "        \"depth_percent\": float(depth_percent),\n",
    "        \"needle\": needle,\n",
    "        \"model_response\": response,\n",
    "        \"score\": score,\n",
    "        \"test_duration_seconds\": test_elapsed_time,\n",
    "        \"test_timestamp_utc\": datetime.now(timezone.utc).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S%z\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    testing_results.append(results)\n",
    "    print(f\"-- Test Summary -- \")\n",
    "    print(f\"Duration: {test_elapsed_time:.1f} seconds\")\n",
    "    print(f\"Context: {context_length} tokens\")\n",
    "    print(f\"Depth: {depth_percent}%\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "    model_version = model_name.split(\"/\")[-1]\n",
    "    context_file_location = f'{model_version.replace(\".\", \"_\")}_len_{context_length}_depth_{int(depth_percent*100)}'\n",
    "\n",
    "    results[\"file_name\"] = context_file_location\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"contexts\"):\n",
    "        os.makedirs(\"contexts\")\n",
    "\n",
    "    if not os.path.exists(f\"contexts/{model_version}\"):\n",
    "        os.makedirs(f\"contexts/{model_version}\")\n",
    "\n",
    "    with open(\n",
    "        f\"contexts/{model_version}/{context_file_location}_context.txt\",\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        f.write(context)\n",
    "\n",
    "    # Save the context to file for retesting\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    if not os.path.exists(f\"results/{model_version}\"):\n",
    "        os.makedirs(f\"results/{model_version}\")\n",
    "\n",
    "    # Save the result to file for retesting\n",
    "    p = f\"results/{model_version}/{context_file_location}_results.json\"\n",
    "    print(\"Writing at %s\" % p)\n",
    "    print(p)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return None, generated_prompt\n",
    "\n",
    "s_len = 1\n",
    "e_len = pretrained_len\n",
    "tasks = []\n",
    "for context_length in context_lengths:\n",
    "    print(context_length)\n",
    "    if context_length < s_len or context_length > e_len:\n",
    "        continue\n",
    "    for depth_percent in document_depth_percents:\n",
    "        print(depth_percent)\n",
    "        task = bound_evaluate_and_log(context_length, depth_percent)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a2f8-9020-4828-9f4f-84adafac2cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60aa097-3346-48e4-aa0c-b5a3262da9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9106f27-dbe0-4745-8e23-c95d5d19cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
